{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport os\nimport matplotlib as mpl\nif os.environ.get('DISPLAY','') == '':\n    print('no display found. Using non-interactive Agg backend')\n    mpl.use('Agg')\n# export MPLBACKEND=Agg\nimport os\nimport pdb\nimport numpy as np\nimport sys\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pprint # For feature explainer\n\nimport torch\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\nimport matplotlib.gridspec as gridspec\nfrom collections import defaultdict\nfrom torchvision.utils import save_image\nfrom PIL import Image\n\npp = pprint.PrettyPrinter(indent=4)\n\n\ndef generate_dir_names(dataset, make = True):\n    h_type='cnn';\n    theta_reg_type='grad3';\n    theta_arch='simple';\n    theta_reg_lambda=1e-2;\n    lr=0.001;\n    nconcepts=3;\n    h_sparsity=1e-4;\n    model_path1='models';\n    if h_type == 'input':\n        suffix = '{}_H{}_Th{}_Reg{:0.0e}_LR{}'.format(\n                    theta_reg_type,\n                    h_type,\n                    theta_arch,\n                    theta_reg_lambda,\n                    lr,\n                    )\n    else:\n        suffix = '{}_H{}_Th{}_Cpts{}_Reg{:0.0e}_Sp{}_LR{}'.format(\n                    theta_reg_type,\n                    h_type,\n                    theta_arch,\n                    nconcepts,\n                    theta_reg_lambda,\n                    h_sparsity,\n                    lr,\n                    )\n    log_path1='log';\n    results_path1='out';\n    model_path     = os.path.join(model_path1, dataset, suffix)\n    log_path       = os.path.join(log_path1, dataset, suffix)\n    results_path   = os.path.join(results_path1, dataset, suffix)\n\n    if make:\n        for p in [model_path, results_path]: #, log_path,\n            if not os.path.exists(p):\n                os.makedirs(p)\n\n    return model_path, log_path, results_path\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\n\n### Animation Utils\n\n# animation function\ndef animate_fn(i, xx,yy,Ts, Cs):\n    t = Ts[i]\n    C = Cs[i]\n    cont = plt.contourf(xx, yy, C, 25, cmap = plt.cm.RdBu)\n    plt.title(r'Iter = %i' % t)\n    return cont\n\ndef animate_training(Steps, Cs, X_train, y_train):\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    h = .02  # step size in the mesh\n    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    line, = ax.plot([],[], '-')\n    line2, = ax.plot([],[],'--')\n    ax.set_xlim(np.min(xx), np.max(xx))\n    ax.set_xlim(np.min(yy), np.max(yy))\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n       edgecolors='k')\n    #ax.contourf(xx, yy, Cs[0].reshape(xx.shape))\n\n    anim = animation.FuncAnimation(fig, animate_fn, frames = len(Steps), fargs = (xx,yy,Steps,Cs,), interval = 200, blit = False)\n\n    return anim\n\n# Got these two from scikit learn embedding example\ndef make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(model, X, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    fig, ax = plt.subplots()\n    Z = model(X)\n    Z = Z.data.numpy()\n    C = np.argmax(Z,axis=1)\n    C = C.reshape(xx.shape)\n    out = ax.contourf(xx, yy, C, **params)\n    return out\n\n\ndef plot_embedding(X,y,Xp, title=None):\n    \"\"\" Scale and visualize the embedding vectors \"\"\"\n    x_min, x_max = np.min(Xp, 0), np.max(Xp, 0)\n    Xp = (Xp - x_min) / (x_max - x_min)\n\n    plt.figure(figsize=(20,10))\n    ax = plt.subplot(111)\n    for i in range(Xp.shape[0]):\n        plt.text(Xp[i, 0], Xp[i, 1], str(y[i]),\n                 color=plt.cm.Set1(y[i] / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    if hasattr(offsetbox, 'AnnotationBbox'):\n        # only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(X.shape[0]):\n            dist = np.sum((Xp[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                # don't show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [Xp[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(X[i,:].reshape(28,28), cmap=plt.cm.gray_r),\n                Xp[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n\n\ndef _explain_class(model, x_raw, x,k,typ='pos',thresh = 0.5,recompute=True):\n    \"\"\"\n        Given an input x and class index k, explain f(x) by returning indices of\n        features in x that have highest positive impact on predicting class k.\n    \"\"\"\n    if recompute:\n        y = model(x) # y = self.model(x)\n    B_k = model.params[0,k,:].data.numpy()\n    if typ == 'pos':\n        Mask = (B_k > thresh).astype(np.int).reshape(x.size()).squeeze()\n    elif typ == 'neg':\n        Mask = (B_k < -thresh).astype(np.int).reshape(x.size()).squeeze()\n    else:\n        # Return weights instead of mask\n        return B_k.reshape(x.size()).squeeze()\n    Masked_x = Mask*x_raw.numpy().squeeze()\n    return Masked_x\n\ndef explain_digit(model, x_raw, x, thresh = 0.5, save_path = None):\n    \"\"\"\n        Given an input x, explain f(x) by returning indices of\n        features in x that have highest positive impact on predicting each class.\n\n        x_raw is passed for plotting purposes\n    \"\"\"\n    plt.imshow(x_raw.squeeze().numpy())\n    plt.title('Input:')\n    plt.xticks([])\n    plt.yticks([])\n    if save_path:\n        plt.savefig(save_path+'_input.pdf',  bbox_inches = 'tight', format='pdf', dpi=300)\n    plt.show()\n    y_pred = model(x)\n\n    pred_class = np.argmax(y_pred.data.numpy())\n    print('Predicted: ',pred_class)\n\n    fig, ax = plt.subplots(3,model.dout,figsize=(1.5*model.dout,1.5*3))\n    for i in range(model.dout):\n        #print('Class {}:'.format(i))\n\n        # Positive\n        x_imask = _explain_class(model, x_raw, x,i,typ ='pos', recompute=False, thresh = thresh)\n        ax[0,i].imshow(x_imask)\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title('Class: {}'.format(i))\n\n        # Negative\n        x_imask = _explain_class(model, x_raw, x,i,  typ ='neg', recompute=False, thresh = thresh)\n        ax[1,i].imshow(x_imask)\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n\n        # Combined\n        x_imask = _explain_class(model, x_raw, x,i,  typ ='both', recompute=False)\n        ax[2,i].imshow(x_imask, cmap = plt.cm.RdBu)\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        #print(np.linalg.norm(x_imask))\n        #print(x_imask[:5,:5])\n\n        if i == 0:\n            ax[0,0].set_ylabel('Pos. Feats.')\n            ax[1,0].set_ylabel('Neg. Feats.')\n            ax[2,0].set_ylabel('Combined')\n\n\n    if save_path:\n        plt.savefig(save_path + '_expl.pdf', bbox_inches = 'tight', format='pdf', dpi=300)\n    plt.show()\n\ndef plot_text_explanation(words, values, n_cols = 6, save_path = None):\n    import seaborn as sns\n    # Get some pastel shades for the colors\n    #colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n    cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n\n    n_rows = int(min(len(values), len(words)) / n_cols) + 1\n\n    # Plot bars and create text labels for the table\n    if type(words) is str:\n        words = words.split(' ')\n\n    cellcolours = np.empty((n_rows, n_cols), dtype='object')\n    celltext    = np.empty((n_rows, n_cols), dtype='object')\n\n    for r in range(n_rows):\n        for c in range(n_cols):\n            idx = (r * n_cols + c)\n            val =  values[idx] if (idx < len(values)) else 0\n            cellcolours[r,c] = cmap(val)\n            celltext[r,c] = words[idx] if (idx < len(words)) else ''\n\n    fig, ax = plt.subplots()#figsize=(n_cols, n_rows))\n\n    # Hide axes\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n\n    # Add a table at the bottom of the axes\n    tab = plt.table(cellText=celltext,\n                          cellColours = cellcolours,\n                          rowLabels=None,\n                          rowColours=None,\n                          colLabels=None,\n                          cellLoc='center',\n                          loc='center')\n\n    for key, cell in tab.get_celld().items():\n        cell.set_linewidth(0)\n\n    tab.set_fontsize(14)\n    tab.scale(1.5, 1.5)  # may help\n\n    # Adjust layout to make room for the table:\n    #plt.subplots_adjust(left=0.2, bottom=0.2)\n\n    #plt.ylabel(\"Loss in ${0}'s\".format(value_increment))\n    plt.yticks([])\n    plt.xticks([])\n    plt.title('')\n    plt.axis('off')\n    plt.grid('off')\n    if save_path:\n        plt.savefig(save_path + '_expl.pdf', bbox_inches = 'tight', format='pdf', dpi=300)\n    plt.show()\n\n\nclass FeatureInput_Explainer():\n    \"\"\"\n        Explainer for classification task models that take vector of features\n        as input and and return class probs.\n\n        Arguments:\n\n\n    \"\"\"\n    def __init__(self, feature_names, binary = False, sort_rows = True, scale_values = True):\n        super(FeatureInput_Explainer, self).__init__()\n        self.features = feature_names\n        self.binary   = binary # Whether it is a binary classif task\n        self.sort_rows = sort_rows\n        self.scale_values = scale_values\n\n    def explain(self, model, x, thresh = 0.5, save_path = None):\n        np.set_printoptions(threshold=15, precision = 2)\n        #print('input: {}'.format(x.data.numpy()))\n        print('Input:')\n        pp.pprint(dict(zip(self.features, *x.data.numpy())))\n        print('')\n\n        np.set_printoptions()\n        y_pred = model(x)\n        pred_class = np.argmax(y_pred.data.numpy())\n        print('Predicted: ',pred_class)\n\n        # Get data-dependent params\n        B = model.thetas[0,:,:].data.numpy() # class x feats\n\n        Pos_Mask = (B > thresh).astype(np.int)#.reshape(x.size()).squeeze()\n        Neg_Mask = (B < thresh).astype(np.int)#.reshape(x.size()).squeeze()\n\n        title = r'Relevance Score $\\theta(x)$' + (' (Scaled)' if self.scale_values else '')\n        if self.binary:\n            d = dict(zip(self.features, B[:,0])) # Change to B[0,:] when B model is truly binary\n            A = plot_dependencies(d, title= title,\n                                     scale_values = self.scale_values,\n                                     sort_rows = self.sort_rows)\n        else:\n            Pos_Feats = {}\n            for k in range(B.shape[0]):\n                d = dict(zip(self.features, B[k,:])) # Change to B[0,:] when B model is truly binary\n                A = plot_dependencies(d, title= title,\n                                         scale_values = self.scale_values,\n                                         sort_rows = self.sort_rows)\n                Neg_Feats = list(compress(self.features, B[k,:] < -thresh))\n                Pos_Feats = list(compress(self.features, B[k,:] > thresh))\n                print('Class:{:5} Neg: {}, Pos: {}'.format(k, ','.join(Neg_Feats), ','.join(Pos_Feats)))\n        if save_path:\n            plt.savefig(save_path, bbox_inches = 'tight', format='pdf', dpi=300)\n        plt.show()\n        print('-'*60)\n\n    def _explain_class(self, x_raw, x,k,typ='pos',feat_names = None, thresh = 0.5,recompute=True):\n        \"\"\"\n            Given an input x and class index k, explain f(x) by returning indices of\n            features in x that have highest positive impact on predicting class k.\n        \"\"\"\n        if recompute:\n            y = model(x) # y = self.model(x)\n        B_k = model.params[0,k,:].data.numpy()\n        #print((B_k > thresh).astype(np.int))\n\n        if feat_names and typ == 'pos':\n            # Return masked features instead of values\n            return list(compress(feat_names, B_k > thresh ))\n        elif feat_names and typ == 'neg':\n            return list(compress(feat_names, B_k < thresh ))\n        if typ == 'pos':\n            Mask = (B_k > thresh).astype(np.int).reshape(x.size()).squeeze()\n        elif typ == 'neg':\n            Mask = (B_k < -thresh).astype(np.int).reshape(x.size()).squeeze()\n        else:\n            # Return weights instead of mask\n            return B_k.reshape(x.size()).squeeze()\n\n        Masked_x = Mask*x_raw.numpy().squeeze()\n        return Masked_x\n\ndef plot_dependencies(dictionary_values,\n                      pos_color=\"#ff4d4d\",\n                      negative_color=\"#3DE8F7\",\n                      reverse_values=False,\n                      sort_rows =True,\n                      scale_values = True,\n                      title=\"\",\n                      fig_size=(4, 4),\n                      ax = None,\n                      x = None,\n                      digits = 1, prediction_text = None,\n                      show_table = False, ax_table = None):\n    \"\"\" This function was adapted form the fairml python package\n\n        x needed only if show_table = True\n\n        digits: (int) significant digits in table\n    \"\"\"\n    # add check to make sure that dependence features are not zeros\n    if np.sum(np.array(dictionary_values.values())) == 0.0:\n        print(\"Feature dependence for all attributes equal zero.\"\n              \" There is nothing to plot here. \")\n        return None\n\n    column_names = list(dictionary_values.keys())\n    coefficient_values = list(dictionary_values.values())\n\n    # get maximum\n    maximum_value = np.absolute(np.array(coefficient_values)).max()\n    if scale_values:\n        coefficient_values = ((np.array(coefficient_values) / maximum_value) * 100)\n\n    if sort_rows:\n        index_sorted = np.argsort(np.array(coefficient_values))\n    else:\n        index_sorted = range(len(coefficient_values))[::-1]\n\n    sorted_column_names = list(np.array(column_names)[index_sorted])\n    sorted_column_values = list(np.array(coefficient_values)[index_sorted])\n    pos = np.arange(len(sorted_column_values)) + 0.7\n\n    # rearrange this at some other point.\n    def assign_colors_to_bars(array_values,\n                              pos_influence=pos_color,\n                              negative_influence=negative_color,\n                              reverse=reverse_values):\n\n        # if you want the colors to be reversed for positive\n        # and negative influences.\n        if reverse:\n            pos_influence, negative_influence = (negative_influence,\n                                                 pos_influence)\n\n        # could rewrite this as a lambda function\n        # but I understand this better\n        def map_x(x):\n            if x > 0:\n                return pos_influence\n            else:\n                return negative_influence\n        bar_colors = list(map(map_x, array_values))\n        return bar_colors\n\n    bar_colors = assign_colors_to_bars(coefficient_values, reverse=True)\n    bar_colors = list(np.array(bar_colors)[index_sorted])\n\n    #pdb.set_trace()\n    if ax is None and not show_table:\n        #pdb.set_trace()\n        fig, ax = plt.subplots(figsize=fig_size)\n    elif ax is None and show_table:\n        fig, axes = plt.subplots(1, 2, figsize=fig_size)\n        ax_table, ax = axes\n\n    ax.barh(pos, sorted_column_values, align='center', color=bar_colors)\n    ax.set_yticks(pos)\n    ax.set_yticklabels(sorted_column_names)\n    if scale_values:\n        ax.set_xlim(-105, 105)\n    else:\n        pass\n        #ax.set_xlim(-1.05, 1.05)\n    if title:\n        ax.set_title(\"{}\".format(title))\n\n    if show_table and ax_table:\n        cell_text = [[('%1.' + str(digits) + 'f') % v] for v in x]\n        if prediction_text is None:\n            ax_table.axis('off')\n        else:\n            print('here')\n            ax_table.set_xticklabels([])\n            ax_table.set_yticklabels([])\n            ax_table.set_yticks([])\n            ax_table.set_xticks([])\n            for side in ['top', 'right', 'bottom', 'left']:\n                ax_table.spines[side].set_visible(False)\n            ax_table.set_xlabel(prediction_text)\n\n        ax_table.table(cellText=cell_text,\n                                  rowLabels=sorted_column_names[::-1],\n                                  rowColours=bar_colors[::-1],\n                                  colLabels=None,#['Value'],\n                                  colWidths=[1],\n                                  loc='left', cellLoc = 'right',\n                                  bbox=[0.2, 0.025, 0.95, 0.95])\n        ax_table.set_title('Input Value')\n        return ax, ax_table\n\n    return ax\n\n\ndef plot_theta_stability(model, input, pert_type = 'gauss', noise_level = 0.5,\n                         samples = 5, save_path = None):\n    \"\"\" Test stability of relevance scores theta for perturbations of an input.\n\n        If model is of type 1 (i.e. theta is of dimension nconcepts x nclass), visualizes\n        the perturbations of dependencies with respect to predicted class.\n\n        If model is of type 1/3 (theta is a vector of size nconcepts), then there's only\n        one dimension of theta to visualize.\n\n        Args:\n            model (GSENN): GSENN model.\n            inputs (list of tensors): Inputs over which stability will be tested. First one is \"base\" input.\n\n        Returns:\n            stability: scalar.\n\n        Displays plot also.\n\n    \"\"\"\n    def gauss_perturbation(x, scale = 1):\n        noise = Variable(scale*torch.randn(x.size()), volatile = True)\n        if x.is_cuda:\n            noise = noise.cuda()\n        return x + noise\n\n    model.eval()\n\n    # Generate perturbations\n    inputs = [input]\n    for i in range(samples):\n        inputs.append(gauss_perturbation(input, scale=noise_level))\n\n    fig, ax = plt.subplots(2,len(inputs),figsize=(2*len(inputs),1.5*3))\n\n    # Map Them\n    thetas = []\n    dists  = []\n    for i,x in enumerate(inputs):\n        pred = model(x)\n        ax[0,i].imshow(x.data.cpu().numpy().squeeze())#, cmap = 'Greys', interpolation = 'nearest')\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        if i == 0:\n            ax[0,i].set_title('Original'.format(i))\n        else:\n            ax[0,i].set_title('Perturbation {}'.format(i))\n\n\n\n        theta = model.thetas.data.cpu().numpy().squeeze()\n        if theta.shape[1] > 1:\n            # Means this is model 1, scalar h and theta_i vector-sized. Choose which one ti visualize\n            klass = pred.data.max(1)[1] # Predicted class\n            deps = theta[:,klass].squeeze()\n            thetas.append(deps)\n        else:\n            deps = theta\n            thetas.append(deps)\n        classes = ['C' + str(i) for i in range(theta.shape[0])]\n        d = dict(zip(classes, deps))\n        A = plot_dependencies(d, title= 'Dependencies', sort_rows = False, ax = ax[1,i])\n        #ax[1,i].locator_params(axis = 'y', nbins=10)\n\n        # max_yticks = 10\n        # yloc = plt.MaxNLocator(max_yticks)\n        # ax[1,i].yaxis.set_major_locator(yloc)\n        #print(thetas[-1])\n        if i > 0:\n            dists.append(np.linalg.norm(thetas[0] - deps))\n\n    dists = np.array(dists)\n    plt.tight_layout()\n    #print(dists.max())\n    if save_path:\n        plt.savefig(save_path, bbox_inches = 'tight', format='pdf', dpi=300)\n    #plt.show(block=False)\n\n\ndef concept_grid(model, data_loader, cuda=False, top_k = 6, layout = 'vertical', return_fig=False, save_path = None):\n    \"\"\"\n        Finds examples in data_loader that are most representatives of concepts.\n\n        For scalar concepts, activation is simply the value of the concept.\n        For vector concepts, activation is the norm of the concept.\n\n    \"\"\"\n    print('Warning: make sure data_loader passed to this function doesnt shuffle data!!')\n    all_norms = []\n    num_concepts = model.conceptizer.nconcept\n    # concept_dim  = model.conceptizer.dout\n\n    for i in range(num_concepts):\n      path = '/content/drive/MyDrive/RPC_ANTE/cifar10/test/cifar_{}'.format(i+1)\n      if not os.path.exists(path):\n          os.mkdir(path)\n\n    top_activations = {k: np.array(top_k*[-1000.00]) for k in range(num_concepts)}\n    top_examples = {k: top_k*[None] for k in range(num_concepts)}\n    all_activs = []\n    all_concepts = defaultdict(list)\n    all_logits = defaultdict(list)\n    all_s_logits = defaultdict(list)\n    all_pen_layers = defaultdict(list)\n    # for idx, (data, target) in enumerate(data_loader):\n    for i, (data, indexes) in enumerate(data_loader, 0):\n        # get the inputs\n        target = Variable(indexes.long())\n        if cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        pred1, pred2, _, pen_layer = model(data)\n        concepts = model.concepts.data\n        #pdb.set_trace()\n        #concepts[concepts < 0] = 0.0 # This is unncessary if output of H is designed to be > 0.\n        if concepts.shape[-1] > 1:\n            print('ERROR')\n            print(asd.asd)\n            activations = np.linalg.norm(concepts, axis = 2)\n        else:\n            activations = concepts\n\n            indexes_np = indexes.cpu().numpy()\n            activations_np = activations.cpu().numpy()\n            logits_np = pred1.detach().cpu().numpy()\n            s_logits_np = pred2.detach().cpu().numpy()\n            pen_layer_np = pen_layer.detach().cpu().numpy()\n\n            # print (indexes_np)\n            # print (activations_np.shape)\n            # print (pred1.shape)\n            # print (pred2.shape)\n            # print (pen_layer.shape)\n            # print (concepts.shape)\n            for i in range(indexes_np.shape[0]):\n                # all_concepts[indexes[i]].append(concepts[i].squeeze())\n                # print (indexes_np[i])\n                # print (all_concepts.keys())\n                all_concepts[indexes_np[i]].append(activations_np[i].squeeze())\n                all_logits[indexes_np[i]].append(logits_np[i])\n                all_s_logits[indexes_np[i]].append((s_logits_np[i]))\n                all_pen_layers[indexes_np[i]].append((pen_layer_np[i]))\n\n        all_activs.append(activations)\n        # if idx == 10:\n        #     break\n\n    all_activs = torch.cat(all_activs)\n    # print (all_activs.shape)\n    # print (all_concepts.keys())\n    # print(all_logits.keys())\n    # print(all_s_logits.keys())\n    # print(all_pen_layers.keys())\n\n    pickle.dump(all_concepts, open(\"all_concepts\", \"wb\"))\n    pickle.dump(all_logits, open(\"all_logits\", \"wb\"))\n    pickle.dump(all_s_logits, open(\"all_s_logits\", \"wb\"))\n    pickle.dump(all_pen_layers, open(\"all_pen_layers\", \"wb\"))\n    print (\"Done\")\n    # sys.exit()\n\n    top_activations, top_idxs = torch.topk(all_activs, top_k, 0)\n    top_activations = top_activations.squeeze().t()\n    top_idxs = top_idxs.squeeze().t()\n    # top_examples = {}\n    top_examples = defaultdict(list)\n    # print (top_idxs)\n    for i in range(num_concepts):\n        # top_examples[i] = data_loader.data[top_idxs[i]]\n        # top_examples[i] = data_loader.dataset[top_idxs[i]]\n        for j in top_idxs[i]:\n            get_image,_ = data_loader.dataset[j]\n            # print (get_image.shape)\n            # print (get_image.min(), get_image.max())\n            get_image_1 = (get_image - get_image.min())/(get_image.max() - get_image.min())\n            # print(get_image_1.min(), get_image_1.max())\n            top_examples[i].append(get_image_1.permute(1, 2, 0).numpy())\n    #top_examples =\n\n\n    # Before, i was doing this manually :\n        # for i in range(activations.shape[0]):\n        #     #pdb.set_trace()\n        #     for j in range(num_concepts):\n        #         min_val  = top_activations[j].min()\n        #         min_idx  = top_activations[j].argmin()\n        #         if activations[i,j] >  min_val:\n        #             # Put new one in place of min\n        #             top_activations[j][min_idx]  = activations[i,j]\n        #             top_examples[j][min_idx] = data[i, :, :, :].data.numpy().squeeze()\n        #     #pdb.set_trace()\n    # for k in range(num_concepts):\n    #     #print(k)\n    #     Z = [(v,e) for v,e in sorted(zip(top_activations[k],top_examples[k]),  key=lambda x: x[0], reverse = True)]\n    #     top_activations[k], top_examples[k] = zip(*Z)\n\n    if layout == 'horizontal':\n        num_cols = top_k\n        num_rows = num_concepts\n        figsize=(num_cols, 1.2*num_rows)\n    else:\n        num_cols = num_concepts\n        num_rows = top_k\n        figsize=(1.4*num_cols, num_rows)\n\n    fig, axes  = plt.subplots(figsize=figsize, nrows=num_rows, ncols=num_cols )\n\n    for i in range(num_concepts):\n        for j in range(top_k):\n            pos = (i,j) if layout == 'horizontal' else (j,i)\n\n            l = i*top_k + j\n            # print(i,j)\n            # print(top_examples[i][j].shape)\n            # print (top_examples[i][j])\n            axes[pos].imshow(top_examples[i][j], cmap='Greys',  interpolation='nearest')\n            plt.imsave('test/cifar_' + str(i + 1) + '/' + str(j) + '.png',\n                       top_examples[i][j])\n            # img = Image.fromarray(top_examples[i][j], 'RGB')\n            # img.save('/om2/user/anirbans/CIFAR10/test/cifar_'+ str(i+1)+'/'+str(j)+'.png')\n            # save_image(top_examples[i][j], os.path.join('/om2/user/anirbans/CIFAR10/scripts/test/cifar_'+str(i+1), str(j)+'.png'))\n            if layout == 'vertical':\n                axes[pos].axis('off')\n                if j == 0:\n                    axes[pos].set_title('Cpt {}'.format(i+1), fontsize = 24)\n            else:\n                axes[pos].set_xticklabels([])\n                axes[pos].set_yticklabels([])\n                axes[pos].set_yticks([])\n                axes[pos].set_xticks([])\n                for side in ['top', 'right', 'bottom', 'left']:\n                    axes[i,j].spines[side].set_visible(False)\n                if i == 0:\n                    axes[pos].set_title('Proto {}'.format(j+1))\n                if j == 0:\n                    axes[pos].set_ylabel('Concept {}'.format(i+1), rotation = 90)\n\n    print('Done')\n\n    # cols = ['Prot.{}'.format(col) for col in range(1, num_cols + 1)]\n    # rows = ['Concept # {}'.format(row) for row in range(1, num_rows + 1)]\n    #\n    # for ax, col in zip(axes[0], cols):\n    #     ax.set_title(col)\n    #\n    # for ax, row in zip(axes[:,0], rows):\n    #     ax.set_ylabel(row, rotation=0, size='large')\n    #plt.tight_layout()\n\n    if layout == 'vertical':\n        fig.subplots_adjust(wspace=0.01, hspace=0.1)\n    else:\n        fig.subplots_adjust(wspace=0.1, hspace=0.01)\n\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches = 'tight', format='pdf', dpi=300)\n    plt.show()\n    if return_fig:\n        return fig, axes\n\n\ndef plot_prob_drop(attribs, prob_drop, save_path = None):\n\n    ind = np.arange(len(attribs))\n    column_names = [str(j) for j in range(1,22)]\n\n    width = 0.65\n\n    fig, ax1 = plt.subplots(figsize=(8,4))\n\n    color1 = '#377eb8'\n    ax1.bar(ind+width+0.35, attribs, 0.45, color=color1)\n    ax1.set_ylabel(r'Feature Relevance $\\theta(x)_i$',color=color1, fontsize = 14)\n    #ax1.set_ylim(-1,1)\n    ax1.set_xlabel('Feature')\n    ax1.tick_params(axis='y', colors=color1)\n\n\n    color2 = '#ff7f00'\n    ax2 = ax1.twinx()\n    ax2.ticklabel_format(style='sci',scilimits=(-2,2),axis = 'y')\n    ax2.plot(ind+width+0.35, prob_drop, 'bo', linestyle='dashed', color=color2)\n    ax2.set_ylabel('Probability Drop', color = color2, fontsize = 14)\n    ax2.tick_params(axis='y', colors=color2)\n\n\n    ax1.set_xticks(ind+width+(width/2))\n    ax1.set_xticklabels(column_names)\n\n    fig.tight_layout()\n    if save_path:\n        plt.savefig(save_path, bbox_inches = 'tight', format='pdf', dpi=300)\n\n    plt.show()\n\ndef noise_stability_plots(model, dataset, cuda, save_path):\n    # find one example of each digit:\n    examples = {}\n    i = 0\n    while (not len(examples.keys()) == 10) and (i < len(dataset)):\n        if dataset[i][1] not in examples:\n            examples[dataset[i][1]] = dataset[i][0].view(1,1,28,28)\n        i += 1\n\n    for i in range(10):\n        x = Variable(examples[i], volatile = True)\n        if cuda:\n            x = x.cuda()\n        plot_theta_stability(model, x, noise_level = 0.5,\n                save_path=save_path + '/noise_stability_{}.pdf'.format(i))\n\ndef plot_digit(x, ax = None, greys = True):\n    if ax is None:\n        fig, ax = plt.subplots()\n    if type(x) is torch.Tensor:\n        x = x.numpy()\n    if x.ndim > 2:\n        x = x.squeeze()\n    if greys:\n        ax.imshow(x, cmap='Greys')\n    else:\n        ax.imshow(x)\n    ax.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n","metadata":{"_uuid":"ab08f64d-96a6-4f31-81e0-3584bcc7e4a7","_cell_guid":"f36fc38c-2d77-44f9-824b-41bdf5f9bdff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T11:57:04.432179Z","iopub.execute_input":"2024-12-24T11:57:04.432459Z","iopub.status.idle":"2024-12-24T11:57:11.035453Z","shell.execute_reply.started":"2024-12-24T11:57:04.432441Z","shell.execute_reply":"2024-12-24T11:57:11.034308Z"}},"outputs":[{"name":"stdout","text":"no display found. Using non-interactive Agg backend\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating A Framework for Learning Ante-hoc Explainable Models via Concepts.\nCopyright (C) 2022 Anirban Sarkar <anirbans@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\n# Standard Imports\nimport sys, os\nimport numpy as np\nimport pdb\nimport pickle\nimport argparse\nimport operator\nimport matplotlib\n# import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Torch Imports\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.datasets as dset\nfrom torchvision.datasets import CIFAR10, ImageFolder\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.utils.data.dataloader as dataloader\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport glob\n# # Imports from my other repos\n# from robust_interpret.explainers import gsenn_wrapper\n# from robust_interpret.utils import lipschitz_boxplot, lipschitz_argmax_plot\n\n# Local imports\n#from SENN.arglist import get_senn_parser #parse_args as parse_senn_args\n#from SENN.models import GSENN\n#from SENN.conceptizers import image_fcc_conceptizer, image_cnn_conceptizer, input_conceptizer, image_resnet_conceptizer, EfficientNet, DenseNet\n# from SENN.conceptizers import *\n\n#from SENN.parametrizers import image_parametrizer, torchvision_parametrizer, vgg_parametrizer\n#from SENN.aggregators import linear_scalar_aggregator, additive_scalar_aggregator\n#from SENN.trainers import HLearningClassTrainer, VanillaClassTrainer, GradPenaltyTrainer\n#from SENN.utils import plot_theta_stability, generate_dir_names, noise_stability_plots, concept_grid\n#from SENN.eval_utils import estimate_dataset_lipschitz\nfrom torch.utils.data import DataLoader, Dataset\n# from prettytable import PrettyTable\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#os.environ['CUDA_VISIBLE_DEVICES']='6,7'\n# Custom Dataset Class\nclass CustomImageDataset(Dataset):\n    def __init__(self, data_dir, split='train', transform=None):\n        \"\"\"\n        Args:\n            data_dir (str): Path to the main data directory.\n            split (str): Dataset split, either 'train', 'val', or 'test'.\n            transform (callable, optional): A function/transform to apply on the images.\n        \"\"\"\n        self.data_dir = os.path.join(data_dir, split)\n        self.transform = transform\n        \n        # Initialize lists to hold image paths and corresponding labels\n        self.image_paths = []\n        self.labels = []\n        \n        # Load images and labels\n        self._load_data()\n\n    def _load_data(self):\n        # For each class (NORMAL, PNEUMONIA), get the image paths\n        for label, class_name in enumerate(['NORMAL', 'PNEUMONIA']):\n            class_folder = os.path.join(self.data_dir, class_name)\n            image_paths = glob.glob(os.path.join(class_folder, '*.jpeg'))  # Change to appropriate extension if needed\n            self.image_paths.extend(image_paths)\n            self.labels.extend([label] * len(image_paths))  # Assign label (0 for NORMAL, 1 for PNEUMONIA)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Get the image path and label\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        # Open image\n        image = Image.open(img_path).convert('RGB')\n\n        # Apply the transformations (resize, normalization, etc.)\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Function to load data\ndef load_custom_data_from_drive(data_dir='/kaggle/input/chest-xray-pneumonia/', batch_size=64, num_workers=1, resize=(224, 224)):\n    \"\"\"\n    Load dataset from Google Drive directory containing train, test, and val folders.\n    Each of these folders contains NORMAL and PNEUMONIA subfolders.\n    \"\"\"\n    # Define transformations for training, validation, and test\n    transform = transforms.Compose([\n        transforms.Resize(resize),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # Adjust if needed for your dataset\n    ])\n\n    # Load datasets manually using the CustomImageDataset\n    train_data = CustomImageDataset(data_dir, split='train', transform=transform)\n    val_data = CustomImageDataset(data_dir, split='val', transform=transform)\n    test_data = CustomImageDataset(data_dir, split='test', transform=transform)\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader, train_data, test_data\ndef load_custom_data(data_dir='/kaggle/input/chest-xray-pneumonia/', valid_size=0.1, shuffle=True, resize=None, random_seed=2008, batch_size=64, num_workers=1):\n  \"\"\"\n  Load images from the custom dataset with train, val, and test splits from the given directory.\n    \n  Args:\n  - data_dir (str): The base directory where the train, val, and test folders are located.\n  - valid_size (float): Proportion of the training set to be used as validation.\n  - shuffle (bool): Whether to shuffle the dataset before splitting.\n  - resize (tuple): Resize the images if specified.\n  - random_seed (int): Random seed for reproducibility.\n  - batch_size (int): Batch size for data loading.\n  - num_workers (int): Number of workers for data loading.\n    \n  Returns:\n  - train_loader (DataLoader): DataLoader for training data.\n  - valid_loader (DataLoader): DataLoader for validation data.\n  - test_loader (DataLoader): DataLoader for testing data.\n  - train_dataset (Dataset): The training dataset.\n  - test_dataset (Dataset): The testing dataset.\n  \"\"\"\n\n  # Transformations for the training and testing datasets\n  transform_train = transforms.Compose([\n      transforms.RandomCrop(32, padding=4) if resize is None else transforms.Resize(resize),\n      transforms.RandomHorizontalFlip(),\n      transforms.ToTensor(),\n      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n  ])\n\n  transform_test = transforms.Compose([\n      transforms.Resize(resize) if resize else transforms.ToTensor(),\n      transforms.ToTensor(),\n      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n  ])\n\n  # Load the dataset using ImageFolder\n  train_data_dir = os.path.join(data_dir, 'train')\n  val_data_dir = os.path.join(data_dir, 'val')\n  test_data_dir = os.path.join(data_dir, 'test')\n\n  # Dataset loading from train, val, and test folders\n  train_dataset = datasets.ImageFolder(train_data_dir)\n  test_dataset = datasets.ImageFolder(test_data_dir)\n  val_dataset = datasets.ImageFolder(val_data_dir)\n\n  # Number of training samples\n  num_train = len(train_dataset)\n  indices = list(range(num_train))\n\n  # Split the dataset into training and validation subsets\n  split = int(np.floor(valid_size * num_train))\n  train_idx, valid_idx = indices[split:], indices[:split]\n\n  # Create samplers for train, validation, and test\n  if shuffle:\n      np.random.seed(random_seed)\n      np.random.shuffle(indices)\n\n  train_sampler = SubsetRandomSampler(train_idx)\n  valid_sampler = SubsetRandomSampler(valid_idx)\n\n  # DataLoader creation\n  dataloader_args = dict(batch_size=batch_size, num_workers=num_workers)\n\n  train_loader = DataLoader(train_dataset, sampler=train_sampler, **dataloader_args)\n  valid_loader = DataLoader(train_dataset, sampler=valid_sampler, **dataloader_args)\n  test_loader = DataLoader(test_dataset, shuffle=False, **dataloader_args)\n\n  return train_loader, valid_loader, test_loader, train_dataset, test_dataset\n\ndef parse_args():\n    senn_parser = get_senn_parser()\n\n    ### Local ones\n    parser = argparse.ArgumentParser(parents =[senn_parser],add_help=False,\n        description='Interpteratbility robustness evaluation on MNIST')\n\n    # #setup\n    parser.add_argument('-d','--datasets', nargs='+',\n                        default = ['heart', 'ionosphere', 'breast-cancer','wine','heart',\n                        'glass','diabetes','yeast','leukemia','abalone'], help='<Required> Set flag')\n    parser.add_argument('--lip_calls', type=int, default=10,\n                        help='ncalls for bayes opt gp method in Lipschitz estimation')\n    parser.add_argument('--lip_eps', type=float, default=0.01,\n                        help='eps for Lipschitz estimation')\n    parser.add_argument('--lip_points', type=int, default=100,\n                        help='sample size for dataset Lipschitz estimation')\n    parser.add_argument('--optim', type=str, default='gp',\n                        help='black-box optimization method')\n\n    #####\n\n    args = parser.parse_args()\n    print(\"\\nParameters:\")\n    for attr, value in sorted(args.__dict__.items()):\n        print(\"\\t{}={}\".format(attr.upper(), value))\n\n    return args\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(\"Total Trainable Params: {}\".format(total_params))\n    return total_params\n\ndef main():\n    #args = parse_args()\n\tseed = 2018;\n    np.random.seed(seed)\n\t\n    torch.manual_seed(seed)\n    nclasses = 10\n    theta_dim = nclasses\n\ttheta_arch= 'simple'\n    if (theta_arch == 'simple') or ('vgg' in theta_arch):\n        H, W = 224, 224\n    else:\n        # Need to resize to have access to torchvision's models\n        H, W = 224, 224\n    input_dim = H*W\n\n    model_path, log_path, results_path = generate_dir_names('cifar')\n    batch_size1=128;\n\tnum_worker1=0;\n    train_loader, valid_loader, test_loader, train_tds, test_tds = load_custom_data_from_drive(\n                        batch_size=batch_size1,num_workers=num_workers1,\n                        resize=(H,W)\n                        )\n    h_type='cnn';\n    if h_type == 'input':\n        conceptizer  = input_conceptizer()\n        nconcepts = input_dim + int(not args.nobias)\n    elif h_type == 'cnn':\n        concept_dim=1;\n        # biase. They treat it like any other concept.\n        #args.nconcepts +=     int(not args.nobias)\n        # conceptizer  = image_cnn_conceptizer(args.input_dim, args.nconcepts, args.concept_dim, nchannel = 3) #, sparsity = sparsity_l)\n        conceptizer = image_resnet_conceptizer(input_dim, nconcepts, nclasses,concept_dim, nchannel = 3)\n        # conceptizer = EfficientNet(args.input_dim, args.nconcepts, args.nclasses, args.concept_dim, nchannel = 3)\n        # conceptizer = DenseNet(args.input_dim, args.nconcepts, args.nclasses, args.concept_dim, nchannel = 3)\n    else:\n        #args.nconcepts +=     int(not args.nobias)\n        conceptizer  = image_fcc_conceptizer(input_dim, nconcepts, concept_dim, nchannel = 3) #, sparsity = sparsity_l)\n\n    positive_theta=False;\n    if theta_arch == 'simple':\n        parametrizer = image_parametrizer(input_dim, nconcepts, theta_dim, nchannel = 3, only_positive =positive_theta)\n    elif 'vgg' in theta_arch:\n        parametrizer = vgg_parametrizer(input_dim, nconcepts, theta_dim, arch = theta_arch, nchannel = 3, only_positive = positive_theta) #torchvision.models.alexnet(num_classes = args.nconcepts*args.theta_dim)\n    else:\n        parametrizer = torchvision_parametrizer(input_dim,nconcepts, theta_dim, arch = theta_arch, nchannel = 3, only_positive = positive_theta) #torchvision.models.alexnet(num_classes = args.nconcepts*args.theta_dim)\n\n\n    aggregator   = additive_scalar_aggregator(nconcepts, concept_dim, nclasses)\n\n    # model        = GSENN(conceptizer, parametrizer, aggregator) #, learn_h = args.train_h)\n    model        = GSENN(conceptizer, aggregator)\n\n    theta_reg_type='grad3';\n    if theta_reg_type in ['unreg','none', None]:\n        trainer = VanillaClassTrainer(model)\n    elif theta_reg_type == 'grad1':\n        trainer = GradPenaltyTrainer(model, args, typ = 1)\n    elif theta_reg_type == 'grad2':\n        trainer = GradPenaltyTrainer(model, args, typ = 2)\n    elif theta_reg_type == 'grad3':\n        trainer = GradPenaltyTrainer(model,typ = 3)\n    elif theta_reg_type == 'crosslip':\n        trainer = CLPenaltyTrainer(model, args)\n    else:\n        raise ValueError('Unrecoginzed theta_reg_type')\n    epochs=10;\n    #if args.train or not args.load_model or (not os.path.isfile(os.path.join(model_path,'model_best.pth.tar'))):\n    trainer.train(train_loader, valid_loader, epochs = epochs, save_path = model_path+\"/newmodel/\")\n    trainer.plot_losses(save_path=results_path+\"/newmodel/results/\")\n    # else:\n    #     checkpoint = torch.load(os.path.join(model_path,'model_best.pth.tar'), map_location=lambda storage, loc: storage)\n    #     checkpoint.keys()\n    #     print(checkpoint.keys())\n    #     model = checkpoint['model']\n    #     # model.load_state_dict(checkpoint['state_dict'])\n    #     #optimizer = checkpoint['optimizer']\n    #     #print(optimizer)\n    #     #optimizer.load_state_dict(checkpoint['optimizer'])\n    #     # epoch = checkpoint['epoch']\n    #     # print(epoch)\n    #     # if epoch !=3:\n    #     #   print(\"hi\")\n    #     #   trainer =  GradPenaltyTrainer(model, args,typ = 3)\n    #     #   trainer.train(train_loader, valid_loader, epochs = args.epochs, save_path = model_path)\n    #     #   trainer.plot_losses(save_path=results_path)\n    #     # else:\n    #     #   trainer =  VanillaClassTrainer(model, args) # arbtrary trained, only need to compuyte val acc\n\n    # model.eval()\n\n    #  #Check accuracy with the best model\n    # checkpoint = torch.load(os.path.join(model_path,'model_best.pth.tar'), map_location=lambda storage, loc: storage)\n    # checkpoint.keys()\n    # model = checkpoint['model']\n    # trainer =  VanillaClassTrainer(model, args)\n    # trainer.evaluate(test_loader)\n\n    # # pytorch_total_params = sum(p.numel() for p in model.parameters())\n    # pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    # print (pytorch_total_params)\n    # count_parameters(model)\n\n    # All_Results = {}\n\n    # 0. Concept Grid for Visualization\n\tcuda=False;\n    concept_grid(model, test_loader, top_k = 10, cuda = cuda, save_path = results_path + '/concept_grid.pdf')\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport pdb\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass additive_scalar_aggregator(nn.Module):\n    \"\"\" Linear aggregator for interpretable classification.\n\n        Aggregates a set of concept representations and their\n        scores, generates a prediction probability output from them.\n\n        Args:\n            cdim (int):     input concept dimension\n            nclasses (int): number of target classes\n\n        Inputs:\n            H:   H(x) vector of concepts (b x k x 1) [TODO: generalize to set maybe?]\n            Th:  Theta(x) vector of concept scores (b x k x nclass)\n\n        Output:\n            - Vector of class probabilities (b x o_dim)\n\n        TODO: add number of layers as argument, construct in for?\n    \"\"\"\n\n    def __init__(self, nconcepts, cdim, nclasses):\n        super(additive_scalar_aggregator, self).__init__()\n        self.nconcepts = nconcepts #Number of concepts\n        self.cdim      = cdim       # Dimension of each concept\n        self.nclasses  = nclasses   # Numer of output classes\n        self.binary = (nclasses == 1)\n        self.linear = nn.Linear(self.nconcepts, self.nclasses)\n\n    # def forward(self, H, Th):\n    def forward(self, H):\n        # print (H.shape)\n        # assert H.size(-2) == Th.size(-2), \"Number of concepts in H and Th don't match\"\n        assert H.size(-1) == 1, \"Concept h_i should be scalar, not vector sized\"\n        # assert Th.size(-1) == self.nclasses, \"Wrong Theta size\"\n        # combined = torch.bmm(Th.transpose(1,2), H).squeeze(dim=-1)\n        # print (self.cdim)\n        if self.binary:\n            out = F.sigmoid(combined)\n        else:\n            # out =  F.log_softmax(combined, dim = 1)\n            out = F.log_softmax(self.linear(H.view(-1,self.nconcepts)))\n        # print (out.shape)\n        return out\n\n\nclass linear_scalar_aggregator(nn.Module):\n    \"\"\" Linear aggregator for interpretable classification.\n\n        Aggregates a set of concept representations and their\n        scores, generates a prediction probability output from them.\n\n        Args:\n            cdim (int):     input concept dimension\n            nclasses (int): number of target classes\n\n        Inputs:\n            H:   H(x) vector of concepts (b x k x 1) [TODO: generalize to set maybe?]\n            Th:  Theta(x) vector of concept scores (b x k x nclass)\n\n        Output:\n            - Vector of class probabilities (b x o_dim)\n\n        TODO: add number of layers as argument, construct in for?\n    \"\"\"\n\n    def __init__(self, cdim, nclasses, softmax_pre = True):\n        super(linear_scalar_aggregator, self).__init__()\n        self.cdim      = cdim       # Dimension of each concept\n        self.nclasses  = nclasses   # Numer of output classes\n\n        self.linear = nn.Linear(din, dout)\n        self.softmax_pre = softmax_pre\n\n    def forward(self, H, Th):\n        assert H.size(-2) == Th.size(-2), \"Number of concepts in H and Th don't match\"\n        assert H.size(-1) == 1, \"Concept h_i should be scalar, not vector sized\"\n        assert Th.size(-1) == self.nclasses, \"Wrong Theta size\"\n\n        # Previously:\n        #combined = torch.bmm(H.transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n\n        # New - do softmax before aggregation:\n        if self.softmax_pre:\n            H_soft = F.log_softmax(self.linear(H), dim=2)\n            combined = torch.bmm(H_soft.transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n        else:\n            combined = torch.bmm(self.linear(H).transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n            combined = F.log_softmax(combined)\n\n        return combined\n\n\n\nclass linear_vector_aggregator(nn.Module):\n    \"\"\" Linear aggregator for interpretable classification.\n\n        Aggregates a set of concept representations and their\n        scores, generates a prediction probability output from them.\n\n        Args:\n            din (int): input concept dimension\n            dout (int): output dimension (num classes)\n\n        Inputs:\n            H:  H(x) matrix of concepts (b x k x c_dim) [TODO: generalize to set maybe?]\n            Th:  Theta(x) vector of concept scores (b x k x 1) (TODO: generalize to multi-class scores)\n\n        Output:\n            - Vector of class probabilities (b x o_dim x 1)\n\n        TODO: add number of layers as argument, construct in for?\n    \"\"\"\n\n    def __init__(self, din, dout, softmax_pre = True):\n        super(linear_vector_aggregator, self).__init__()\n        self.din    = din\n        self.dout   = dout\n        self.linear = nn.Linear(din, dout)\n        self.softmax_pre = softmax_pre\n\n    def forward(self, H, Th):\n        assert(H.size(-2) == Th.size(-2))\n        assert(H.size(-1) == self.din)\n\n        # Previously:\n        #combined = torch.bmm(H.transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n\n        # New - do softmax before aggregation:\n        if self.softmax_pre:\n            H_soft = F.log_softmax(self.linear(H), dim=2)\n            combined = torch.bmm(H_soft.transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n        else:\n            combined = torch.bmm(self.linear(H).transpose(1,2), Th).squeeze(dim=-1) # Don't want to squeeze batch dim!\n            combined = F.log_softmax(combined)\n\n        return combined\n\n\n# Set version:\n#     def forward(self, h, th):\n#         assert(h.size(0) == th.size(0))\n#         agg = torch.zeros(self.dout, 1)\n#         for k in len(h):\n#             agg += self.concept_encoder(h[k],th[k])\n#         return agg\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport pdb\nimport numpy as np\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Function\n# from torch.legacy import nn as nn_legacy\nfrom torch.autograd import Variable\n\nimport torch.utils.model_zoo as model_zoo\n# from custom_modules import SequentialWithArgs, FakeReLU\n\n#===============================================================================\n#==========================      REGULARIZERS        ===========================\n#===============================================================================\n\n# From https://discuss.pytorch.org/t/how-to-create-a-sparse-autoencoder-neural-network-with-pytorch/3703\nclass L1Penalty(Function):\n    @staticmethod\n    def forward(ctx, input, l1weight):\n        ctx.save_for_backward(input)\n        ctx.l1weight = l1weight\n        return input\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_variables\n        grad_input = input.clone().sign().mul(self.l1weight)\n        grad_input += grad_output\n        return grad_input\n\n\n#===============================================================================\n#=======================       MODELS FOR IMAGES       =========================\n#===============================================================================\n\nclass input_conceptizer(nn.Module):\n    \"\"\" Dummy conceptizer for images: each input feature (e.g. pixel) is a concept.\n\n        Args:\n            indim (int): input concept dimension\n            outdim (int): output dimension (num classes)\n\n        Inputs:\n            x: Image (b x c x d x d) or Generic tensor (b x dim)\n\n        Output:\n            - H:  H(x) matrix of concepts (b x dim  x 1) (for images, dim = x**2)\n                  or (b x dim +1 x 1) if add_bias = True\n    \"\"\"\n\n    def __init__(self, add_bias = True):\n        super(input_conceptizer, self).__init__()\n        self.add_bias = add_bias\n        self.learnable = False\n\n    def forward(self, x):\n        if len(list(x.size())) == 4:\n            # This is an images\n            out = x.view(x.size(0), x.size(-1)**2, 1)\n        else:\n            out = x.view(x.size(0), x.size(1), 1)\n        if self.add_bias:\n            pad = (0,0,0,1) # Means pad to next to last dim, 0 at beginning, 1 at end\n            out = F.pad(out, pad, mode = 'constant', value = 1)\n        return out\n\n\nclass AutoEncoder(nn.Module):\n    \"\"\"\n        A general autoencoder meta-class with various penalty choices.\n\n        Takes care of regularization, etc. Children of the AutoEncoder class\n        should implement encode() and decode() functions.\n        Encode's output should be same size/dim as decode input and viceversa.\n        Ideally, AutoEncoder should not need to do any resizing (TODO).\n\n    \"\"\"\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        # self.sparsity = sparsity is not None\n        # self.l1weight = sparsity if (sparsity) else 0.1\n\n    def forward(self, x):\n        encoded, logits, out = self.encode(x)\n        # if self.sparsity:\n        #     #encoded = L1Penalty.apply(encoded, self.l1weight)    # Didn't work\n        decoded = self.decode(encoded)\n        return encoded, logits, decoded.view_as(x), out\n\nclass image_fcc_conceptizer(AutoEncoder):\n    \"\"\" MLP-based conceptizer for concept basis learning.\n\n        Args:\n            din (int): input size\n            nconcept (int): number of concepts\n            cdim (int): concept dimension\n\n        Inputs:\n            x: Image (b x c x d x d)\n\n        Output:\n            - Th: Tensor of encoded concepts (b x nconcept x cdim)\n    \"\"\"\n\n    def __init__(self, din, nconcept, cdim): #, sparsity = None):\n        super(image_fcc_conceptizer, self).__init__()\n        self.din      = din        # Input dimension\n        self.nconcept = nconcept   # Number of \"atoms\"/concepts\n        self.cdim     = cdim       # Dimension of each concept\n        self.learnable = True\n\n        self.encoder = nn.Sequential(\n            nn.Linear(28 * 28, 128), nn.ReLU(True),\n            nn.Linear(128, 64), nn.ReLU(True),\n            nn.Linear(64, 12), nn.ReLU(True),\n            nn.Linear(12, nconcept*cdim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(nconcept*cdim, 12), nn.ReLU(True),\n            nn.Linear(12, 64), nn.ReLU(True),\n            nn.Linear(64, 128), nn.ReLU(True),\n            nn.Linear(128, 28 * 28),\n            nn.Tanh()\n        )\n    def encode(self, x):\n        x = x.view(x.size(0), -1)\n        encoded = self.encoder(x).view(-1, self.nconcept, self.cdim)\n        return encoded\n\n    def decode(self, z):\n        decoded = self.decoder(z.view(-1, self.cdim*self.nconcept))\n        return decoded\n\n    # def forward(self, x):\n    #     x_dims = x.size()\n    #     x = x.view(x.size(0), -1)\n    #     encoded = self.encoder(x).view(-1, self.nconcept, self.cdim)\n    #     decoded = self.decoder(encoded.view(-1, self.cdim*self.nconcept)).view(x_dims)\n    #     return encoded, decoded\n    #\n\n\nclass image_cnn_conceptizer(AutoEncoder):\n    \"\"\" CNN-based conceptizer for concept basis learning.\n\n        Args:\n            din (int): input size\n            nconcept (int): number of concepts\n            cdim (int): concept dimension\n\n        Inputs:\n            x: Image (b x c x d x d)\n\n        Output:\n            - Th: Tensor of encoded concepts (b x nconcept x cdim)\n    \"\"\"\n\n    def __init__(self, din, nconcept, cdim=None, nchannel =1): #, sparsity = None):\n        super(image_cnn_conceptizer, self).__init__()\n        self.din      = din        # Input dimension\n        self.nconcept = nconcept   # Number of \"atoms\"/concepts\n        self.cdim     = cdim       # Dimension of each concept\n        self.nchannel = nchannel\n        self.learnable = True\n        self.add_bias = False\n        self.dout     = int(np.sqrt(din)//4 - 3*(5-1)//4) # For kernel = 5 in both, and maxppol stride = 2 in both\n\n        # Encoding\n        self.conv1  = nn.Conv2d(nchannel,10, kernel_size=5)    # b, 10, din - (k -1),din - (k -1)\n        # after pool layer (functional)                        # b, 10,  (din - (k -1))/2, idem\n        self.conv2  = nn.Conv2d(10, nconcept, kernel_size=5)   # b, 10, (din - (k -1))/2 - (k-1), idem\n        # after pool layer (functional)                        # b, 10,  din/4 - 3(k-1))/4, idem\n        self.linear1 = nn.Linear(self.dout**2, self.cdim)       # b, nconcepts, cdim\n        # self.linear2 = nn.Linear(self.dout**2, self.cdim)\n        # self.linear3 = nn.Linear(self.nconcept, 10)\n        self.linear2 = nn.Linear(self.nconcept * (self.dout**2), 128)\n        self.linear3 = nn.Linear(128,64)\n        self.linear4 = nn.Linear(64,20)\n        self.linear5 = nn.Linear(20,10)\n\n        # Decoding\n        self.unlinear = nn.Linear(self.cdim,self.dout**2)                # b, nconcepts, dout*2\n        self.deconv3  = nn.ConvTranspose2d(nconcept, 16, 5, stride = 2)  # b, 16, (dout-1)*2 + 5, 5\n        self.deconv2  = nn.ConvTranspose2d(16, 8, 5)                     # b, 8, (dout -1)*2 + 9\n        self.deconv1  = nn.ConvTranspose2d(8, nchannel, 2, stride=2, padding=1) # b, nchannel, din, din\n\n\n    def encode(self, x):\n        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # x.to(device)\n        # x = x.to('cpu')\n        # print (x.type())\n        p = self.conv1(x)\n        # print (p.type())\n        p       = F.relu(F.max_pool2d(p, 2))\n        # p       = F.relu(F.max_pool2d(self.conv1(x), 2))\n        p       = F.relu(F.max_pool2d(self.conv2(p), 2))\n        # print (p.shape)\n        encoded = self.linear1(p.view(-1, self.nconcept, self.dout**2))\n        # intermid = self.linear2(p.view(-1, self.nconcept, self.dout**2))\n        # print (intermid.shape)\n        # logits = self.linear3(intermid.view(-1, self.nconcept))\n        intermid = self.linear2(p.view(-1, self.nconcept*(self.dout**2)))\n        intermid1 = F.relu(self.linear3(intermid.view(-1,128)))\n        intermid2 = F.relu(self.linear4(intermid1.view(-1,64)))\n        logits = self.linear5(intermid2.view(-1,20))\n\n\n        # print (self.din)\n        # print (self.nconcept)\n        # print (self.cdim)\n        # print (self.dout)\n        # print (p.shape)\n        # print (encoded.shape)\n        # print (intermid.shape)\n        # print (logits.shape)\n        return encoded, logits\n\n    def decode(self, z):\n        q       = self.unlinear(z).view(-1, self.nconcept, self.dout, self.dout)\n        q       = F.relu(self.deconv3(q))\n        q       = F.relu(self.deconv2(q))\n        decoded = F.tanh(self.deconv1(q))\n        return decoded\n    #\n    #\n    # def forward(self, x):\n    #     \n    #\n    #     # Encoding\n    #     p       = F.relu(F.max_pool2d(self.conv1(x), 2))\n    #     p       = F.relu(F.max_pool2d(self.conv2(p), 2))\n    #     encoded = self.linear(p.view(-1, self.nconcept, 4 * 4))\n    #\n    #     # Decoding\n    #     q       = self.unlinear(encoded).view(-1, self.nconcept, 4, 4)\n    #     q       = F.relu(self.deconv3(q))\n    #     q       = F.relu(self.deconv2(q))\n    #     decoded = F.tanh(self.deconv1(q))\n    #     # decoded =\n    #     #\n    #     # encoded  = self.Linear(conv_out.view())\n    #     # decoded = self.decoder(encoded)\n    #     # print(encoded.size())\n    #     # encoded = encoded.view(x.size(0), self.nconcept, self.cdim)\n    #     return encoded, decoded\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass image_resnet_conceptizer(AutoEncoder):\n    \"\"\" CNN-based conceptizer for concept basis learning.\n\n        Args:\n            din (int): input size\n            nconcept (int): number of concepts\n            cdim (int): concept dimension\n\n        Inputs:\n            x: Image (b x c x d x d)\n\n        Output:\n            - Th: Tensor of encoded concepts (b x nconcept x cdim)\n    \"\"\"\n\n    def __init__(self, din, nconcept, nclass, cdim=None, nchannel =1, block=Bottleneck, num_blocks=[3,4,23,3]): #, sparsity = None):\n        super(image_resnet_conceptizer, self).__init__()\n        self.din      = din        # Input dimension\n        self.nconcept = nconcept   # Number of \"atoms\"/concepts\n        self.nclass = nclass       # Number of classes\n        self.cdim     = cdim       # Dimension of each concept\n        self.nchannel = nchannel\n        self.learnable = True\n        self.add_bias = False\n        self.dout     = int(np.sqrt(din)//4 - 3*(5-1)//4) # For kernel = 5 in both, and maxppol stride = 2 in both\n        self.concept_in_planes = 64\n\n        # Encoding\n        # self.conv1  = nn.Conv2d(nchannel,10, kernel_size=5)    # b, 10, din - (k -1),din - (k -1)\n        # # after pool layer (functional)                        # b, 10,  (din - (k -1))/2, idem\n        # self.conv2  = nn.Conv2d(10, nconcept, kernel_size=5)   # b, 10, (din - (k -1))/2 - (k-1), idem\n        # # after pool layer (functional)                        # b, 10,  din/4 - 3(k-1))/4, idem\n        # self.linear1 = nn.Linear(self.dout**2, self.cdim)       # b, nconcepts, cdim\n        # # self.linear2 = nn.Linear(self.dout**2, self.cdim)\n        # # self.linear3 = nn.Linear(self.nconcept, 10)\n        # self.linear2 = nn.Linear(self.nconcept * (self.dout**2), 128)\n        # self.linear3 = nn.Linear(128,64)\n        # self.linear4 = nn.Linear(64,20)\n        # self.linear5 = nn.Linear(20,10)\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.conc_layer1 = self._make_layer_concept(block, 64, num_blocks[0], stride=1)\n        self.conc_layer2 = self._make_layer_concept(block, 128, num_blocks[1], stride=2)\n        self.conc_layer3 = self._make_layer_concept(block, 256, num_blocks[2], stride=2)\n        self.conc_layer4 = self._make_layer_concept(block, 512, num_blocks[3], stride=2)\n        # self.conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True)\n        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conc_linear1 = nn.Linear(512*block.expansion, self.nconcept)\n        self.conc_linear2 = nn.Linear(512*block.expansion, self.nclass)\n        # self.conc_linear3 = nn.Linear(256, num_concepts)\n\n        # Decoding\n        self.unlinear = nn.Linear(self.cdim,self.dout**2)                # b, nconcepts, dout*2\n        self.deconv3  = nn.ConvTranspose2d(nconcept, 16, 5, stride = 2)  # b, 16, (dout-1)*2 + 5, 5\n        self.deconv2  = nn.ConvTranspose2d(16, 8, 5)                     # b, 8, (dout -1)*2 + 9\n        self.deconv1  = nn.ConvTranspose2d(8, nchannel, 2, stride=2, padding=1) # b, nchannel, din, din\n\n        # self.unlinear = nn.Linear(self.nconcept, 1024)\n        # self.deconv_6 = nn.ConvTranspose2d(1024, 512, kernel_size = 5, stride = 2, padding = 1, bias = True) # 8, 7,7\n        # # self.deconv_6 = nn.ConvTranspose2d(512, 512, kernel_size = 7, stride = 2, padding = 0, bias = True) # 8, 7,7\n        # self.deconv_5 = nn.ConvTranspose2d(512, 256, kernel_size = 7, stride = 2, padding = 0, bias = True) # 8, 7,7\n        # self.deconv_4 = nn.ConvTranspose2d(256, 128, kernel_size = 5, stride = 2, padding = 0, bias = True) # 8, 7,7\n        # self.deconv_3 = nn.ConvTranspose2d(128, 64, kernel_size = 7, stride = 2, padding = 0, bias = True) # 16, 9, 9\n        # self.deconv_2 = nn.ConvTranspose2d(64, 32, kernel_size = 5, stride = 2, padding = 1, bias = True) # 3, 17, 17\n        # self.deconv_1 = nn.ConvTranspose2d(32, 3, kernel_size = 4, stride = 2, padding = 0, bias = True) # 8, 32, 32\n\n\n    def _make_layer_concept(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        # print (strides)\n        for stride in strides:\n            layers.append(block(self.concept_in_planes, planes, stride))\n            self.concept_in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # x.to(device)\n        # x = x.to('cpu')\n        # print (x.type())\n\n        # p = self.conv1(x)\n        # # print (p.type())\n        # p       = F.relu(F.max_pool2d(p, 2))\n        # # p       = F.relu(F.max_pool2d(self.conv1(x), 2))\n        # p       = F.relu(F.max_pool2d(self.conv2(p), 2))\n        # # print (p.shape)\n        # encoded = self.linear1(p.view(-1, self.nconcept, self.dout**2))\n        # # intermid = self.linear2(p.view(-1, self.nconcept, self.dout**2))\n        # # print (intermid.shape)\n        # # logits = self.linear3(intermid.view(-1, self.nconcept))\n        # intermid = self.linear2(p.view(-1, self.nconcept*(self.dout**2)))\n        # intermid1 = F.relu(self.linear3(intermid.view(-1,128)))\n        # intermid2 = F.relu(self.linear4(intermid1.view(-1,64)))\n        # logits = self.linear5(intermid2.view(-1,20))\n\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.conc_layer1(out)\n        out = self.conc_layer2(out)\n        out = self.conc_layer3(out)\n        out = self.conc_layer4(out)\n        # out = F.relu(self.conv2(out))\n        out = F.avg_pool2d(out, 4)\n        # out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        # print (out.shape)\n        encoded = F.relu(self.conc_linear1(out)).view(-1, self.nconcept, self.cdim)\n        logits = self.conc_linear2(out)\n\n        # print (self.din)\n        # print (self.nconcept)\n        # print (self.cdim)\n        # print (self.dout)\n        # print (p.shape)\n        # print (encoded.shape)\n        # print (intermid.shape)\n        # print (logits.shape)\n        return encoded, logits, out\n\n    def decode(self, z):\n        q       = self.unlinear(z).view(-1, self.nconcept, self.dout, self.dout)\n        q       = F.relu(self.deconv3(q))\n        q       = F.relu(self.deconv2(q))\n        decoded = F.tanh(self.deconv1(q))\n        return decoded\n        # print (z.shape)\n        # recon_out = self.unlinear(z.squeeze()).view(-1, 1024, 1, 1)\n        # recon_out = F.relu(self.deconv_6(recon_out))\n        # recon_out = F.relu(self.deconv_5(recon_out))\n        # recon_out = F.relu(self.deconv_4(recon_out))\n        # recon_out = F.relu(self.deconv_3(recon_out))\n        # recon_out = F.relu(self.deconv_2(recon_out))\n        # recon_out = F.relu(self.deconv_1(recon_out))\n        # return recon_out\n    #\n    #\n    # def forward(self, x):\n    #     \n    #\n    #     # Encoding\n    #     p       = F.relu(F.max_pool2d(self.conv1(x), 2))\n    #     p       = F.relu(F.max_pool2d(self.conv2(p), 2))\n    #     encoded = self.linear(p.view(-1, self.nconcept, 4 * 4))\n    #\n    #     # Decoding\n    #     q       = self.unlinear(encoded).view(-1, self.nconcept, 4, 4)\n    #     q       = F.relu(self.deconv3(q))\n    #     q       = F.relu(self.deconv2(q))\n    #     decoded = F.tanh(self.deconv1(q))\n    #     # decoded =\n    #     #\n    #     # encoded  = self.Linear(conv_out.view())\n    #     # decoded = self.decoder(encoded)\n    #     # print(encoded.size())\n    #     # encoded = encoded.view(x.size(0), self.nconcept, self.cdim)\n    #     return encoded, decoded\n\ndef ResNet18(din, nconcept, cdim=None, nchannel =1):\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3,4,6,3])\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3,8,36,3])\n\n\n#EfficientNet\n\ndef swish(x):\n    return x * x.sigmoid()\n\n\ndef drop_connect(x, drop_ratio):\n    keep_ratio = 1.0 - drop_ratio\n    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n    mask.bernoulli_(keep_ratio)\n    x.div_(keep_ratio)\n    x.mul_(mask)\n    return x\n\n\nclass SE(nn.Module):\n    '''Squeeze-and-Excitation block with Swish.'''\n\n    def __init__(self, in_channels, se_channels):\n        super(SE, self).__init__()\n        self.se1 = nn.Conv2d(in_channels, se_channels,\n                             kernel_size=1, bias=True)\n        self.se2 = nn.Conv2d(se_channels, in_channels,\n                             kernel_size=1, bias=True)\n\n    def forward(self, x):\n        out = F.adaptive_avg_pool2d(x, (1, 1))\n        out = swish(self.se1(out))\n        out = self.se2(out).sigmoid()\n        out = x * out\n        return out\n\n\nclass Block(nn.Module):\n    '''expansion + depthwise + pointwise + squeeze-excitation'''\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 expand_ratio=1,\n                 se_ratio=0.,\n                 drop_rate=0.):\n        super(Block, self).__init__()\n        self.stride = stride\n        self.drop_rate = drop_rate\n        self.expand_ratio = expand_ratio\n\n        # Expansion\n        channels = expand_ratio * in_channels\n        self.conv1 = nn.Conv2d(in_channels,\n                               channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n\n        # Depthwise conv\n        self.conv2 = nn.Conv2d(channels,\n                               channels,\n                               kernel_size=kernel_size,\n                               stride=stride,\n                               padding=(1 if kernel_size == 3 else 2),\n                               groups=channels,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n        # SE layers\n        se_channels = int(in_channels * se_ratio)\n        self.se = SE(channels, se_channels)\n\n        # Output\n        self.conv3 = nn.Conv2d(channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        # Skip connection if in and out shapes are the same (MV-V2 style)\n        self.has_skip = (stride == 1) and (in_channels == out_channels)\n\n    def forward(self, x):\n        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))\n        out = swish(self.bn2(self.conv2(out)))\n        out = self.se(out)\n        out = self.bn3(self.conv3(out))\n        if self.has_skip:\n            if self.training and self.drop_rate > 0:\n                out = drop_connect(out, self.drop_rate)\n            out = out + x\n        return out\n\n\nclass EfficientNet(AutoEncoder):\n    def __init__(self, din, nconcept, num_classes=10, cdim=None, nchannel=3):\n        super(EfficientNet, self).__init__()\n        self.din = din\n        self.num_concept = nconcept\n        self.cdim     = cdim\n        self.learnable = True\n        self.dout     = int(np.sqrt(din)//4 - 3*(5-1)//4) # For kernel = 5 in both, and maxppol stride = 2 in both\n        self.cfg = {\n                    'num_blocks': [1, 2, 2, 3, 3, 4, 1],\n                    'expansion': [1, 6, 6, 6, 6, 6, 6],\n                    'out_channels': [16, 24, 40, 80, 112, 192, 320],\n                    'kernel_size': [3, 3, 5, 3, 5, 5, 3],\n                    'stride': [1, 2, 2, 2, 1, 2, 1],\n                    'dropout_rate': 0.2,\n                    'drop_connect_rate': 0.2,\n                }\n        self.conv1 = nn.Conv2d(3,\n                               32,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_channels=32)\n        self.linear = nn.Linear(self.cfg['out_channels'][-1], num_classes)\n        self.linear1 = nn.Linear(self.cfg['out_channels'][-1], self.num_concept)\n\n        # Decoding\n        self.unlinear = nn.Linear(self.cdim,self.dout**2)                # b, nconcepts, dout*2\n        self.deconv3  = nn.ConvTranspose2d(nconcept, 16, 5, stride = 2)  # b, 16, (dout-1)*2 + 5, 5\n        self.deconv2  = nn.ConvTranspose2d(16, 8, 5)                     # b, 8, (dout -1)*2 + 9\n        self.deconv1  = nn.ConvTranspose2d(8, nchannel, 2, stride=2, padding=1) # b, nchannel, din, din\n\n    def _make_layers(self, in_channels):\n        layers = []\n        cfg = [self.cfg[k] for k in ['expansion', 'out_channels', 'num_blocks', 'kernel_size',\n                                     'stride']]\n        b = 0\n        blocks = sum(self.cfg['num_blocks'])\n        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):\n            strides = [stride] + [1] * (num_blocks - 1)\n            for stride in strides:\n                drop_rate = self.cfg['drop_connect_rate'] * b / blocks\n                layers.append(\n                    Block(in_channels,\n                          out_channels,\n                          kernel_size,\n                          stride,\n                          expansion,\n                          se_ratio=0.25,\n                          drop_rate=drop_rate))\n                in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        out = swish(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.adaptive_avg_pool2d(out, 1)\n        out = out.view(out.size(0), -1)\n        dropout_rate = self.cfg['dropout_rate']\n        if self.training and dropout_rate > 0:\n            out = F.dropout(out, p=dropout_rate)\n        # print (out.shape)\n        logits = self.linear(out)\n        encoded = self.linear1(out).view(-1, self.num_concept, 1)\n        return encoded, logits\n\n    def decode(self, z):\n        q       = self.unlinear(z).view(-1, self.num_concept, self.dout, self.dout)\n        q       = F.relu(self.deconv3(q))\n        q       = F.relu(self.deconv2(q))\n        decoded = F.tanh(self.deconv1(q))\n        return decoded\n\n\n# def EfficientNetB0():\n#     cfg = {\n#         'num_blocks': [1, 2, 2, 3, 3, 4, 1],\n#         'expansion': [1, 6, 6, 6, 6, 6, 6],\n#         'out_channels': [16, 24, 40, 80, 112, 192, 320],\n#         'kernel_size': [3, 3, 5, 3, 5, 5, 3],\n#         'stride': [1, 2, 2, 2, 1, 2, 1],\n#         'dropout_rate': 0.2,\n#         'drop_connect_rate': 0.2,\n#     }\n#     return EfficientNet(cfg)\n\n\nclass Bottleneck_D(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck_D, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out,x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(AutoEncoder):\n    def __init__(self, din, nconcept, num_classes=10, cdim=None, nchannel=3, block=Bottleneck_D, nblocks=[6,12,24,16], growth_rate=12, reduction=0.5):\n        super(DenseNet, self).__init__()\n        self.din = din\n        self.num_concept = nconcept\n        self.cdim     = cdim\n        self.learnable = True\n        self.dout     = int(np.sqrt(din)//4 - 3*(5-1)//4) # For kernel = 5 in both, and maxppol stride = 2 in both\n        self.growth_rate = growth_rate\n\n        num_planes = 2*growth_rate\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3]*growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n        self.linear1 = nn.Linear(num_planes, self.num_concept)\n\n        # Decoding\n        self.unlinear = nn.Linear(self.cdim,self.dout**2)                # b, nconcepts, dout*2\n        self.deconv3  = nn.ConvTranspose2d(nconcept, 16, 5, stride = 2)  # b, 16, (dout-1)*2 + 5, 5\n        self.deconv2  = nn.ConvTranspose2d(16, 8, 5)                     # b, 8, (dout -1)*2 + 9\n        self.deconv1  = nn.ConvTranspose2d(8, nchannel, 2, stride=2, padding=1) # b, nchannel, din, din\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        logits = self.linear(out)\n        encoded = self.linear1(out).view(-1, self.num_concept, 1)\n        return encoded, logits\n\n    def decode(self, z):\n        q       = self.unlinear(z).view(-1, self.num_concept, self.dout, self.dout)\n        q       = F.relu(self.deconv3(q))\n        q       = F.relu(self.deconv2(q))\n        decoded = F.tanh(self.deconv1(q))\n        return decoded\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n\n\n#===============================================================================\n#=======================       MODELS FOR TEXT       ===========================\n#===============================================================================\n\n\nclass text_input_conceptizer(nn.Module):\n    \"\"\" Dummy conceptizer for images: each token is a concept.\n\n        Args:\n\n        Inputs:\n            x: Text tensor (one hot) (b x 1 x L)\n\n        Output:\n            - H:  H(x) matrix of concepts (b x L x 1) [TODO: generalize to set maybe?]\n    \"\"\"\n\n    def __init__(self):\n        super(text_input_conceptizer, self).__init__()\n        # self.din    = din\n        # self.nconcept = nconcept\n        # self.cdim   = cdim\n\n    def forward(self, x):\n        #return x.view(x.size(0), x.size(-1)**2, 1)\n        #return x.transpose(1,2)._fill(1)\n        return Variable(torch.ones(x.size())).transpose(1,2)\n\n\nclass text_embedding_conceptizer(nn.Module):\n    \"\"\" H(x): word embedding of word x.\n\n        Can be used in a non-learnt way (e.g. if embeddings are already trained)\n        TODO: Should we pass this to parametrizer?\n\n        Args:\n            embeddings (optional): pretrained embeddings to initialize method\n\n        Inputs:\n            x: array of word indices (L X B X 1)\n\n        Output:\n            enc: encoded representation (L x B x D)\n    \"\"\"\n\n    def __init__(self, embeddings = None, train_embeddings = False):\n        super(text_embedding_conceptizer, self).__init__()\n        vocab_size, hidden_dim = embeddings.shape\n        self.hidden_dim = hidden_dim\n        self.embedding_layer = nn.Embedding(vocab_size,hidden_dim)\n        print(type(embeddings))\n        if embeddings is not None:\n            self.embedding_layer.weight.data = torch.from_numpy( embeddings )\n            print('Text conceptizer: initializing embeddings')\n        self.embedding_layer.weight.requires_grad = train_embeddings\n        if embeddings is None and not train_embeddings:\n            print('Warning: embeddings not initialized from pre-trained and train = False')\n\n\n    def forward(self, x):\n        encoded = self.embedding_layer(x.squeeze(1))\n        #encoded = encoded.transpose(0,1) # To have Batch dim again in 0\n        return encoded\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nfrom __future__ import absolute_import, division, unicode_literals\n\nimport time\nimport numpy as np\nimport copy\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\nfrom .utils import AverageMeter\nfrom .models import SENN_FFFC\n\n\ndef tv_reg_loss(model):\n    LAMBDA = 1e-6\n    params = model.params.view(-1,10,28,28)\n    reg_loss = LAMBDA * (\n        torch.sum(torch.abs(params[:, :, :, :-1] - params[:, :, :, 1:])) +\n        torch.sum(torch.abs(params[:, :, :-1, :] - params[:, :, 1:, :]))\n    )\n    return reg_loss\n\nclass PyTorchClassifier(object):\n    \"\"\"\n    Pytorch Classifier class in the style of scikit-learn\n    Classifiers include Logistic Regression and MLP\n\n    This is ported and modified from\n    https://github.com/facebookresearch/SentEval/blob/master/senteval/tools/classifier.py\n\n    \"\"\"\n    def __init__(self, inputdim, nclasses, l2reg=0., batch_size=64, seed=1111,\n                 cuda=False, cudaEfficient=False, nepoches=4, maxepoch=200, print_freq = 1000):\n        # fix seed\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if cuda:\n            torch.cuda.manual_seed(seed)\n        self.cuda = cuda\n        self.inputdim = inputdim\n        self.nclasses = nclasses\n        self.l2reg = l2reg\n        self.batch_size = batch_size\n        self.cudaEfficient = cudaEfficient\n        self.nepoches = nepoches\n        self.maxepoch = maxepoch\n        self.print_freq = print_freq\n\n    def prepare_split(self, X, y, validation_data=None, validation_split=None):\n        # Preparing validation data\n        assert validation_split or validation_data\n        if validation_data is not None:\n            trainX, trainy = X, y\n            devX, devy = validation_data\n        else:\n            permutation = np.random.permutation(len(X))\n            trainidx = permutation[int(validation_split*len(X)):]\n            devidx = permutation[0:int(validation_split*len(X))]\n            trainX, trainy = X[trainidx], y[trainidx]\n            devX, devy = X[devidx], y[devidx]\n\n        if self.cuda and not self.cudaEfficient:\n            trainX = torch.FloatTensor(trainX).cuda()\n            trainy = torch.LongTensor(trainy).cuda()\n            devX = torch.FloatTensor(devX).cuda()\n            devy = torch.LongTensor(devy).cuda()\n        else:\n            trainX = torch.FloatTensor(trainX)\n            trainy = torch.LongTensor(trainy)\n            devX = torch.FloatTensor(devX)\n            devy = torch.LongTensor(devy)\n\n        return trainX, trainy, devX, devy\n\n    def fit(self, train_data, y = None, validation_data=None, validation_split=None,\n            early_stop=True):\n        \"\"\"\n            NOTE: For now I'm keeping the original fit function, which is intended\n            for data given as X,y numpy objects. Maybe merge with fit_dataloader  at some point.\n        \"\"\"\n        self.nepoch = 0\n        bestaccuracy = -1\n        stop_train = False\n        early_stop_count = 0\n\n        # Convert to numpy if necessary\n        if isinstance(train_data, torch.utils.data.Dataset):\n            X = train_data.train_data.float().numpy()\n            y = train_data.train_labels.numpy()\n        else:\n            # Check y is provided\n            X = train_data\n            assert y is not None\n\n\n        # Preparing validation data\n        trainX, trainy, devX, devy = self.prepare_split(X, y, validation_data,\n                                                        validation_split)\n\n        # Training\n        while not stop_train and self.nepoch <= self.maxepoch:\n            self.trainepoch(trainX, trainy, nepoches=self.nepoches)\n            accuracy = self.score(devX, devy)\n            if accuracy > bestaccuracy:\n                bestaccuracy = accuracy\n                bestmodel = copy.deepcopy(self.model)\n            elif early_stop:\n                if early_stop_count >= 5:\n                    stop_train = True\n                early_stop_count += 1\n        self.model = bestmodel\n        return bestaccuracy\n\n\n    def trainepoch(self, X, y, nepoches=1):\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n        self.model.train()\n        end = time.time()\n        for j in range(self.nepoch, self.nepoch + nepoches):\n            permutation = np.random.permutation(len(X))\n            all_costs = []\n            for i in range(0, len(X), self.batch_size):\n                # forward\n                idx = torch.LongTensor(permutation[i:i + self.batch_size])\n                if isinstance(X, torch.cuda.FloatTensor):\n                    idx = idx.cuda()\n                inputs = Variable(X.index_select(0, idx))\n                targets = Variable(y.index_select(0, idx))\n                if self.cudaEfficient:\n                    inputs = inputs.cuda()\n                    targets = targets.cuda()\n                \n                outputs = self.model(inputs.view(inputs.size(0),-1))\n                # loss\n                loss = self.loss_fn(outputs, targets)\n\n                # Regularization\n                if self.reg_loss is not None:\n                    reg_loss = self.reg_loss(self.model)\n                    loss += reg_loss\n\n\n                all_costs.append(loss.data[0])\n                # backward\n                self.optimizer.zero_grad()\n                loss.backward()\n                # Update parameters\n                self.optimizer.step()\n                # measure accuracy and record loss\n                prec1, prec5 = self.accuracy(outputs.data, targets.data, topk=(1, 5))\n                losses.update(loss.data[0], inputs.size(0))\n                top1.update(prec1[0], inputs.size(0))\n                top5.update(prec5[0], inputs.size(0))\n\n                 # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % self.print_freq == 0:\n                    print('Epoch: [{0}][{1}/{2}]  '\n                          'Time {batch_time.val:.2f} ({batch_time.avg:.2f})  '\n                          #'Data {data_time.val:.2f} ({data_time.avg:.2f})  '\n                          'Loss {loss.val:.4f} ({loss.avg:.4f})  '\n                          'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n                          'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                           self.nepoch + j , i, len(X), batch_time=batch_time,\n                           data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n        self.nepoch += nepoches\n\n    def score(self, devX, devy):\n        self.model.eval()\n        correct = 0\n        if self.cuda and (not isinstance(devX, torch.cuda.FloatTensor) or self.cudaEfficient):\n            devX = torch.FloatTensor(devX).cuda()\n            devy = torch.LongTensor(devy).cuda()\n        for i in range(0, len(devX), self.batch_size):\n            Xbatch = Variable(devX[i:i + self.batch_size], volatile=True)\n            ybatch = Variable(devy[i:i + self.batch_size], volatile=True)\n            if self.cudaEfficient:\n                Xbatch = Xbatch.cuda()\n                ybatch = ybatch.cuda()\n            output = self.model(Xbatch.view(Xbatch.size(0),-1))\n            pred = output.data.max(1)[1]\n            correct += pred.long().eq(ybatch.data.long()).sum()\n        accuracy = 1.0*correct / len(devX)\n        return accuracy\n\n    def predict(self, devX):\n        self.model.eval()\n        if not isinstance(devX, torch.cuda.FloatTensor):\n            devX = torch.FloatTensor(devX).cuda()\n        yhat = np.array([])\n        for i in range(0, len(devX), self.batch_size):\n            Xbatch = Variable(devX[i:i + self.batch_size], volatile=True)\n            output = self.model(Xbatch)\n            yhat = np.append(yhat,\n                             output.data.max(1)[1].cpu().numpy())\n        yhat = np.vstack(yhat)\n        return yhat\n\n    def predict_proba(self, devX):\n        self.model.eval()\n        probas = []\n        for i in range(0, len(devX), self.batch_size):\n            Xbatch = Variable(devX[i:i + self.batch_size], volatile=True)\n            if not probas:\n                probas = self.model(Xbatch).data.cpu().numpy()\n            else:\n                probas = np.concatenate(probas,\n                                        self.model(Xbatch).data.cpu().numpy(),\n                                        axis=0)\n        return probas\n\n    def accuracy(self, output, target, topk=(1,)):\n        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n    def accuracy_per_class(self, model, test_loader, classes):\n        \"\"\" TODO: Homogenize with accuracy style and syntax\"\"\"\n        n = len(classes)\n        class_correct = list(0. for i in range(n))\n        class_total = list(0. for i in range(n))\n        confusion_matrix = ConfusionMeter(n) #I have 2 classes here\n        for data in test_loader:\n            inputs, labels = data\n            if self.cuda:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(Variable(inputs))\n            _, predicted = torch.max(outputs.data, 1)\n            c = (predicted == labels).squeeze()\n            confusion_matrix.add(predicted, labels)\n            for i in range(labels.size()[0]):\n                label = labels[i]\n                class_correct[label] += c[i]\n                class_total[label] += 1\n\nclass LogReg(PyTorchClassifier):\n    \"\"\" Logistic Regression with Pytorch \"\"\"\n    def __init__(self, inputdim, nclasses, l2reg=0., batch_size=64,\n                 seed=1111, cuda = False, cudaEfficient=False):\n        super(self.__class__, self).__init__(inputdim, nclasses, l2reg,\n                                             batch_size, seed, cuda, cudaEfficient)\n        self.cuda = cuda\n        self.model = nn.Sequential(\n            nn.Linear(self.inputdim, self.nclasses),\n            )\n        if self.cuda:\n            self.model.cuda()\n        self.loss_fn = nn.CrossEntropyLoss().cuda()\n        self.loss_fn.size_average = False\n        self.optimizer = optim.Adam(self.model.parameters(),\n                                    weight_decay=self.l2reg)\n\nclass MLP(PyTorchClassifier):\n    \"\"\" MLP Regression with Pytorch \"\"\"\n    def __init__(self, inputdim, hiddendim, nclasses, regularization = None,\n                 l2reg=0., batch_size=64,\n                 dropout = False, seed=1111, cuda = False, cudaEfficient=False, nepoches=1, maxepoch=10, print_freq=1000):\n        super(self.__class__, self).__init__(inputdim, nclasses, l2reg,\n                                             batch_size, seed, cuda, cudaEfficient,\n                                             nepoches, maxepoch, print_freq)\n\n        self.hiddendim = hiddendim\n        self.drop_p    = dropout if dropout else 0\n\n        self.model = nn.Sequential(\n            nn.Linear(self.inputdim, self.hiddendim),\n            nn.Dropout(p=self.drop_p),\n            nn.Tanh(),\n            nn.Linear(self.hiddendim, self.nclasses),\n            )\n\n        self.cuda = cuda\n\n        if self.cuda:\n            self.model.cuda()\n\n        self.loss_fn = nn.CrossEntropyLoss().cuda()\n        self.loss_fn.size_average = False\n\n        self.optimizer = optim.Adam(self.model.parameters(),\n                                    weight_decay=self.l2reg)\n\n\nclass SENN_MLP(PyTorchClassifier):\n    def __init__(self, inputdim, hiddendim, nclasses, regularization = None,\n                 l2reg=0., batch_size=64,\n                 dropout = False, seed=1111, cuda = False, cudaEfficient=False, nepoches=1, maxepoch=10, print_freq=1000):\n        super(self.__class__, self).__init__(inputdim, nclasses, l2reg,\n                                             batch_size, seed, cuda, cudaEfficient,\n                                             nepoches, maxepoch, print_freq)\n\n        self.hiddendim = hiddendim\n        self.drop_p    = dropout if dropout else 0\n\n        self.model = nn.Sequential(\n            nn.Linear(self.inputdim, self.hiddendim),\n            nn.Dropout(p=self.drop_p),\n            nn.Tanh(),\n            nn.Linear(self.hiddendim, self.nclasses),\n            )\n\n        self.model = SENN_FFFC(self.inputdim, self.hiddendim, self.nclasses)\n\n        self.cuda = cuda\n\n        if self.cuda:\n            self.model.cuda()\n\n        self.loss_fn = nn.CrossEntropyLoss().cuda()\n        self.loss_fn.size_average = False\n        if not regularization:\n            self.reg_loss = None\n        elif regularization.lower() == 'tv':\n            self.reg_loss = tv_reg_loss\n\n        self.optimizer = optim.Adam(self.model.parameters(),\n                                    weight_decay=self.l2reg)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\n# -*- coding: utf-8 -*-\nimport pdb\nfrom torchvision.utils import save_image\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\n\nDEBUG = False\n\n\n#===============================================================================\n#====================      SIMPLE FC and CNN MODELS  ===========================\n#===============================================================================\n\nclass FCNet(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(784, 548)\n        self.bc1 = nn.BatchNorm1d(548)\n        self.fc2 = nn.Linear(548, 252)\n        self.bc2 = nn.BatchNorm1d(252)\n        self.fc3 = nn.Linear(252, 10)\n\n    def forward(self, x):\n        x = x.view((-1, 784))\n        h = self.fc1(x)\n        h = self.bc1(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.5, training=self.training)\n\n        h = self.fc2(h)\n        h = self.bc2(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.2, training=self.training)\n\n        h = self.fc3(h)\n        out = F.log_softmax(h)\n        return out\n\n\nclass SENNModel(nn.Module):\n    def __init__(self, din, h, dout):\n        self.dout = dout\n        self.din = din\n\n        super(SENNModel, self).__init__()\n\n        self.complex_part = nn.Sequential(\n            nn.Linear(din, 548),\n            nn.BatchNorm1d(548),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(548, 252),\n            nn.BatchNorm1d(252),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(252, din * dout),\n        )\n\n    def forward(self, x):\n        x = x.view((-1, self.din))\n        # print(self.complex_part(x).size())\n        params = self.complex_part(x).view(-1, self.dout, self.din)\n        self.params = params\n        out = torch.bmm(params, x.unsqueeze(2)).squeeze()\n        out = F.softmax(out)\n        return out\n\n    def forward_with_params(self, x):\n        x = x.view((-1, self.din))\n        if self.params is None:\n            raise ValueError('must have run forward first!')\n        out = torch.bmm(self.params.repeat(x.size(0), 1, 1),\n                        x.unsqueeze(2)).squeeze()\n        out = F.softmax(out)\n        return out\n\n\nclass SENN_FFFC(nn.Module):\n    def __init__(self, din, h, dout):\n        self.dout = dout\n        self.din = din\n\n        super(SENN_FFFC, self).__init__()\n\n        self.complex_part = nn.Sequential(\n            nn.Linear(din, 548),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(548, 252),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(252, din * dout),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = x.view((-1, self.din))\n        # print(self.complex_part(x).size())\n        params = self.complex_part(x).view(-1, self.dout, self.din)\n        self.params = params\n        out = torch.bmm(params, x.unsqueeze(2)).squeeze()\n        out = F.log_softmax(out)\n        return out\n\n    def forward_with_params(self, x):\n        x = x.view((-1, self.din))\n        if self.params is None:\n            raise ValueError('must have run forward first!')\n        out = torch.bmm(self.params.repeat(x.size(0), 1, 1),\n                        x.unsqueeze(2)).squeeze()\n        out = F.log_softmax(out)\n        return out\n\n\nclass LENET(nn.Module):\n    def __init__(self, din, h, dout):\n        super(LENET, self).__init__()\n        self.dout = dout\n        self.din = din\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, din * dout)\n\n    def forward(self, x):\n        p = F.relu(F.max_pool2d(self.conv1(x), 2))\n        p = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(p)), 2))\n        p = p.view(-1, 320)\n        #p = F.tanh(self.fc1(p))\n        p = self.fc1(p)\n        out = F.dropout(p, training=self.training).view(-1,\n                                                        self.dout, self.din)\n        return out\n\n\nclass SENN_LENET(nn.Module):\n    def __init__(self, din, h, dout):\n        super(SENN_LENET, self).__init__()\n        self.dout = dout\n        self.din = din\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, din * dout)\n\n    def forward(self, x):\n        p = F.relu(F.max_pool2d(self.conv1(x), 2))\n        p = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(p)), 2))\n        p = p.view(-1, 320)\n        #p = F.tanh(self.fc1(p))\n        p = self.fc1(p)\n        params = F.dropout(\n            p, training=self.training).view(-1, self.dout, self.din)\n        self.params = params\n        out = torch.bmm(params, x.view((-1, self.din, 1))).squeeze()\n        out = F.log_softmax(out)\n        return out\n\n    def forward_with_params(self, x):\n        x = x.view((-1, self.din, 1))\n        if self.params is None:\n            raise ValueError('must have run forward first!')\n        out = torch.bmm(self.params.repeat(x.size(0), 1, 1),\n                        x.unsqueeze(2)).squeeze()\n        out = F.log_softmax(out)\n        return out\n\n\nclass GSENN(nn.Module):\n    ''' Wrapper for GSENN with H-learning'''\n\n    # def __init__(self, conceptizer, parametrizer, aggregator):\n    def __init__(self, conceptizer, aggregator):\n        super(GSENN, self).__init__()\n        # self.dout = dout\n        # self.din = din\n        #self.encoder      = encoder\n        #self.decoder      = decoder\n        self.conceptizer = conceptizer\n        # self.parametrizer = parametrizer\n        self.aggregator = aggregator\n        self.learning_H = conceptizer.learnable\n        self.reset_lstm = hasattr(\n            conceptizer, 'lstm')# or hasattr(parametrizer, 'lstm')\n\n    def forward(self, x):\n        DEBUG = True\n        if DEBUG:\n            print('Input to GSENN:', x.size())\n            #tensor  = x.data[0].cpu().numpy()\n            transform = T.ToPILImage() # make sure tensor is on cpu\n            img = transform(x.data[0])\n            save_image(x.data[0], \"input_image.png\") \n            #img1 = transform(concepts.data[0])\n\n            # display the PIL image\n            img.show()\n            img.save('tobeencoded.jpg')\n            \n\n        # Get interpretable features\n        #h_x         = self.encoder(x.view(x.size(0), -1)).view(-1, self.natoms, self.dout)\n        #self.recons = self.decoder(h_x.view(-1, self.dout*self.natoms))\n        if self.learning_H:\n            h_x, logits_x, x_tilde, pen_layer = self.conceptizer(x)\n            self.recons = x_tilde\n            # if self.sparsity:\n            # Store norm for regularization (done by Trainer)\n            # .mul(self.l1weight) # Save sparsity loss, will be used by trainer\n            self.h_norm_l1 = h_x.norm(p=1)\n        else:\n            h_x = self.conceptizer(\n                autograd.Variable(x.data, requires_grad=False))\n\n        self.concepts = h_x  # .data\n\n        if DEBUG:\n            print('Encoded concepts: ', h_x.size())\n\n            if self.learning_H:\n                print('Decoded concepts: ', x_tilde.size())\n                img1 = transform(x_tilde.data[0])\n                img1.show()\n                tensor =x_tilde.data[0]\n                im1 = img1.save(\"decode.jpg\")\n                tensor = tensor.cpu().mul(255).clamp(0, 255).byte()\n                image = Image.fromarray(tensor.permute(1, 2, 0).numpy())\n\n                # Save as a high-quality JPEG\n                image.save(\"output_image234.jpg\", format=\"JPEG\", quality=100)\n                save_image(x_tilde.data[0], \"decoded_image.png\")\n\n\n        # # Get relevance scores (~thetas)\n        # thetas = self.parametrizer(x)\n\n        # # When theta_i is of dim one, need to add dummy dim\n        # if len(thetas.size()) == 2:\n        #     thetas = thetas.unsqueeze(2)\n\n        # # Store local Parameters\n        # self.thetas = thetas  # .data\n\n        # if DEBUG:\n        #     print('Theta: ', thetas.size())\n\n        if len(h_x.size()) == 4:\n            # Concepts are two-dimensional, so flatten\n            h_x = h_x.view(h_x.size(0), h_x.size(1), -1)\n\n        #print(h_x.shape, thetas.shape)\n\n        # out = self.aggregator(h_x, thetas)\n        out = self.aggregator(h_x)\n\n        # if self.aggregator.nclasses ==  1:\n        #     out = out.squeeze() # Squeeze out single class dimension\n\n        if DEBUG:\n            print('Output: ', out.size())\n\n        return logits_x, out, h_x, pen_layer\n\n    def predict_proba(self, x, to_numpy=False):\n        if type(x) is np.ndarray:\n            to_numpy = True\n            x_t = torch.from_numpy(x).float()\n        elif type(x) is Tensor:\n            x_t = x.clone()\n        else:\n            print(type(x))\n            raise ValueError(\"Unrecognized data type\")\n        out = torch.exp(self(Variable(x_t, volatile=True)).data)\n        if to_numpy:\n            out = out.numpy()\n        return out\n\n    def forward_with_params(self, x):\n        #x = x.view((-1, self.din, 1))\n        if self.learning_H:\n            h_x, _ = self.conceptizer(x)\n        else:\n            h_x = self.conceptizer(x)\n\n        if len(h_x.size()) == 4:\n            # Concepts are two-dimensional, so flatten\n            h_x = h_x.view(h_x.size(0), h_x.size(1), -1)\n\n        if self.thetas is None:\n            raise ValueError('must have run forward first!')\n        if len(self.thetas.size()) == 2:\n            # CAn happen if scalar parametrization and we squeezed out. THough should be correctyed.\n            print('Warning: thetas should always have 3 dim. Check!')\n            thetas = self.thetas.unsqueeze(-1)\n        else:\n            thetas = self.thetas\n\n        out = self.aggregator(h_x, thetas)\n        return out\n    def explain(self, x, y=None, skip_bias=True):\n        \"\"\"\n            Args:\n                - y: class to explain (only useful for multidim outputs), if None, explains predicted\n        \"\"\"\n        out = self.forward(x)\n        theta = self.thetas.data.cpu()\n        print(\"In construction\")\n        if theta.shape[-1] == 1:\n            # single class\n            attr = theta\n        elif type(y) in [list, np.array]:\n            y = torch.Tensor(y)\n            attr = theta.gather(\n                2, y.view(-1, 1).unsqueeze(2).repeat(1, theta.shape[1], theta.shape[2]))[:, :, 0]\n        elif y == 'max':\n            # desired class\n            _, idx = torch.max(out, 1)\n            y = idx.data\n            \n            attr = theta.gather(\n                2, y.view(-1, 1).unsqueeze(2).repeat(1, theta.shape[1], theta.shape[2]))[:, :, 0]\n        elif (y == 'all') or (y is None):\n            # retrieve explanation for all classes\n            attr = theta\n        \n        if (not skip_bias) and self.conceptizer.add_bias:\n            pdb.set_trace()\n            print('here')\n            attr = torch.index_select(\n                attr, -1, torch.LongTensor(range(attr.shape[-1] - 1)))\n            pdb.set_trace()\n        return attr\n\n\n#===============================================================================\n#====================      VGG MODELS FOR CIFAR  ===============================\n#===============================================================================\n\n# Note that these are tailored to native 32x32 resolution\n\ncfg_cifar = {\n    'vgg8':  [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M'],\n    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG_CIFAR(nn.Module):\n    def __init__(self, vgg_name, num_classes=10):\n        super(VGG_CIFAR, self).__init__()\n        self.features = self._make_layers(cfg_cifar[vgg_name])\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(256, 128),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n\ndef vgg11_cifar():\n    return VGG_CIFAR('vgg11')\n\n\ndef vgg13_cifar():\n    return VGG_CIFAR('vgg13')\n\n\ndef vgg16_cifar():\n    return VGG_CIFAR('vgg16')\n\n\ndef vgg19_cifar():\n    return VGG_CIFAR('vgg19')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\" Code for training and evaluating Self-Explaining Neural Networks.\nCopyright (C) 2018 David Alvarez-Melis <dalvmel@mit.edu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License,\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <https://www.gnu.org/licenses/>.\n\"\"\"\n\nimport os\nimport tqdm\nimport time\nimport pdb\nimport shutil\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n# from attrdict import AttrDict\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.utils import save_image\n\n#from fairml import plot_dependencies\n\nfrom .utils import AverageMeter\n\n\n#===============================================================================\n#====================      REGULARIZER UTILITIES    ============================\n#===============================================================================\n\ndef tvd_loss(theta):\n    loss = lambd * (\n        torch.sum(torch.abs(params[:, :, :, :-1] - params[:, :, :, 1:])) +\n        torch.sum(torch.abs(params[:, :, :-1, :] - params[:, :, 1:, :]))\n    )\n    return loss\n\ndef CL_loss(theta, n_class):\n    \"\"\" Cross lipshitc loss from https://arxiv.org/pdf/1705.08475.pdf.\n        Gradient based.\n    \"\"\"\n\n    total = 0\n    for i in range(n_class):\n        for j in range(n_class):\n            total += (grad[i] - grad[j]).norm()**2\n\n    return total/(n_class)\n\ndef compute_jacobian_sum(x, fx):\n    \"\"\" Much faster than compute_jacobian, but only correct for norm L1 stuff\n    since it returns sum of gradients \"\"\"\n    n = x.size(-1)\n    b = x.size(0)\n    c = fx.size(-1)\n    m = fx.size(-2)\n    grad = torch.ones(b, m, c)\n    if x.is_cuda:\n        grad  = grad.cuda()\n    g = torch.autograd.grad(outputs=fx, inputs = x, grad_outputs = grad, create_graph=True, only_inputs=True)[0]#, retain_graph = True)[0] -> not sure this should be true or not. Not needed! Defaults to value of create_graph\n    return g\n\ndef           compute_jacobian(x, fx):\n    # Ideas from https://discuss.pytorch.org/t/clarification-using-backward-on-non-scalars/1059/2\n    \n    \n    \n    b = x.size(0)\n    n = x.size(-1)\n    # if fx.dim() > 1:\n    m = fx.size(-1)\n    # else:\n    #     #e.g. fx = theta and task is binary classifiction, fx is a vector\n    #     m = 1\n    #print(fx.size())\n    #print(b,n,m)\n    J = []\n    for i in range(m):\n        #print(i)\n        grad = torch.zeros(b, m)\n        grad[:,i] = 1\n        if x.is_cuda:\n            grad  = grad.cuda()\n        #print(grad.size(), fx.size(), x.size())\n        #pdb.set_trace()\n        g = torch.autograd.grad(outputs=fx, inputs = x, grad_outputs = grad, create_graph=True, only_inputs=True)[0] #, retain_graph = True)[0]\n        J.append(g.view(x.size(0),-1).unsqueeze(-1))\n    #print(J[0].size())\n    J = torch.cat(J,2)\n    return J\n\n\n#===============================================================================\n#==================================   TRAINERS    ==============================\n#===============================================================================\n\n\ndef save_checkpoint(state, is_best, outpath):\n    # script_dir = dirname(dirname(realpath(__file__)))\n    if outpath == None:\n        outpath = os.path.join(script_dir, 'checkpoints')\n\n    #outdir = os.path.join(outpath, '{}_LR{}_Lambda{}'.format(state['theta_reg_type'],state['lr'],state['theta_reg_lambda']))\n    if not os.path.exists(outpath):\n        os.makedirs(outpath)\n    filename = os.path.join(outpath, 'checkpoint.pth.tar')\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(outpath,'model_best.pth.tar'))\n        sample = open('samplefile.txt', 'a')\n        sample.write(str(state['epoch']) + str(state['best_prec1']) + \"\\n\")\n        sample.close()\n\nclass get_default_transforms(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                              std=[0.5, 0.5, 0.5])\n        # self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n        #                    std=[0.229, 0.224, 0.225])\n    def forward(self, input):\n        for i in range(input.shape[0]):\n            temp = TF.to_pil_image(input[i,:,:,:])\n            temp = TF.to_tensor(temp)\n            input[i,:,:,:] = self.normalize(temp)\n        return input\n\ninput_transform = get_default_transforms()\n\ndef gaussian_noise(inputs, mean=0, stddev=0.18):\n    input = inputs.cpu()\n    input_array = input.data.numpy()\n    print (np.max(input_array), np.min(input_array))\n\n    noise = np.random.normal(loc=mean, scale=stddev, size=np.shape(input_array))\n\n    out = np.clip(np.add(input_array, noise).astype('float32'),0,1)\n   \n\n    output_tensor = torch.from_numpy(out)\n    output_tensor = input_transform(output_tensor)\n    print (output_tensor.max(),output_tensor.min())\n    out_tensor = Variable(output_tensor)\n    out = out_tensor.cuda()\n    out = out.float()\n    return out\n\ndef speckle_noise(inputs, mean=0, scale=0.6):\n    input = inputs.cpu()\n    input_array = input.data.numpy()\n\n    noise = input_array * np.random.normal(size=input_array.shape, scale=0.60)\n\n    out = np.clip(np.add(input_array, noise).astype('float32'),0,1)\n\n    output_tensor = torch.from_numpy(out)\n    output_tensor = input_transform(output_tensor)\n    out_tensor = Variable(output_tensor)\n    out = out_tensor.cuda()\n    out = out.float()\n    return out\n\n# classes = np.array(np.genfromtxt('/home/cs16resch11006/DATASET/AwA2/AwA2/classes.txt', dtype='str'))[:,-1]\n\ndef get_euclidean_dist(curr_labels, class_labels):\n  return np.sqrt(np.sum((curr_labels - class_labels)**2))\n\ndef labels_to_class(pred_labels):\n  predictions = []\n  for i in range(pred_labels.shape[0]):\n    curr_labels = pred_labels[i,:].cpu().detach().numpy()\n    best_dist = sys.maxsize\n    best_index = -1\n    for j in range(predicate_binary_mat.shape[0]):\n      class_labels = predicate_binary_mat[j,:]\n      dist = get_euclidean_dist(curr_labels, class_labels)\n      if dist < best_dist and classes[j] not in train_classes:\n        best_index = j\n        best_dist = dist\n    predictions.append(classes[best_index])\n  return predictions\n\ndef scale(X, x_min, x_max):\n    nom = (X-X.min(axis=0))*(x_max-x_min)\n\n    denom = X.max(axis=0) - X.min(axis=0)\n    denom[denom==0] = 1\n    return x_min + nom/denom \n\nclass ClassificationTrainer():\n    def __init__(self, model, args):\n    #loss_type = 'ce', opt = 'adam', lr = 0.0002, cuda = False, log_interval = 100):\n        #super(ClassificationTrainer, self).__init__()\n        self.model = model\n        self.args = args\n        self.cuda = args.cuda\n\n        self.nclasses = args.nclasses\n\n        if args.nclasses <= 2 and args.objective == 'bce':\n            self.prediction_criterion = F.binary_cross_entropy_with_logits\n        elif args.nclasses <= 2:# THis will be default.  and args.objective == 'bce_logits':\n            self.prediction_criterion = F.binary_cross_entropy # NOTE: This does not do sigmoid itslef\n        elif args.objective == 'cross_entropy':\n            self.prediction_criterion = F.cross_entropy\n        else:\n            self.prediction_criterion = F.nll_loss # NOTE: To be used with output of log_softmax\n\n        # define KL-loss\n        self.criterion_kl = nn.KLDivLoss(size_average=False)\n\n        if args.h_type != 'input':\n            # Means conceptizer will be trained, need reconstruction loss for it\n            self.learning_h = True\n            self.h_reconst_criterion = F.mse_loss  #nn.MSELoss() \n            # if args.h_sparsity != -1:\n            #     print('Will enforce sparsity on h')\n            self.h_sparsity = args.h_sparsity\n        else:\n            self.learning_h = False\n\n        self.loss_history = []  # Will acumulate losse\n        self.print_freq = args.print_freq\n\n        # self.reset_lstm = model.reset_lstm # Trun on when model has an lstm\n\n        #self.outpath = args.save_dir\n\n        optim_betas = (0.9, 0.999)\n\n        if args.opt == 'adam':\n            self.optimizer = optim.Adam(self.model.parameters(), lr= args.lr, betas=optim_betas)\n        elif args.opt == 'rmsprop':\n            #lrD, lrG = 5e-5, 5e-5 # Original WGAN code has 5e-5\n            self.optimizer = optim.RMSprop(self.model.parameters(), lr = args.lr)\n        elif args.opt == 'sgd':\n            self.optimizer = optim.SGD(self.model.parameters(), lr = args.lr, weight_decay = args.weight_decay, momentum=0.9)\n\n\n        if self.cuda:\n            self.model = self.model.cuda()\n            #self.prediction_criterion = self.prediction_criterion.cuda()\n            # if self.learning_h:\n            #     self.h_reconst_criterion  = self.h_reconst_criterion.cuda()\n\n\n\n    def train(self, train_loader, test_loader = None, val_loader = None, epochs = 10, save_path = None):\n        best_prec1 = 0\n        test_prec1 = 0\n        for epoch in range(epochs):\n            # print (len(test_loader))\n            self.train_epoch(epoch, train_loader)\n\n            # if val_loader is not None:\n            #     val_prec1 = self.validate(val_loader) # Ccompytes acc\n            if test_loader is not None:\n                test_prec1 = self.validate(test_loader) # Ccompytes acc\n\n            # Maybe add here computing of empirical robustness?\n\n            # remember best prec@1 and save checkpoint\n            is_best = test_prec1 > best_prec1\n            best_prec1 = max(test_prec1, best_prec1)\n\t\t\th_type='cnn';\n\t\t\ttheta_reg_type='grad3';\n\t\t\ttheta_arch='simple';\n\t\t\ttheta_reg_lambda=1e-2;\n\t\t\tlr=0.001;\n\t\t\tnconcepts=3;\n\t\t\th_sparsity=1e-4;\n\t\t\tmodel_path1='models';\n            if save_path is not None:\n                save_checkpoint({\n                    'epoch': epoch + 1,\n                    'lr': lr,\n                    'theta_reg_lambda': theta_reg_lambda,\n                    'theta_reg_type': theta_reg_type,\n                    'state_dict': self.model.state_dict(),\n                    'best_prec1': best_prec1,\n                    'optimizer' : self.optimizer.state_dict(),\n                    'model': self.model  \n                 }, is_best, save_path)\n\n        print('Training done')\n\n    def train_batch(self):\n        raise NotImplemented('ClassificationTrainers must define their train_batch method!')\n\n\n    def concept_learning_loss(self, inputs, all_losses):\n        recons_loss    = self.h_reconst_criterion(self.model.recons,\n                                Variable(inputs.data, requires_grad = False))\n\n        all_losses['reconstruction'] = recons_loss.item()\n        if self.h_sparsity != -1:\n            sparsity_loss   = self.model.h_norm_l1.mul(self.h_sparsity)\n            all_losses['h_sparsity'] = sparsity_loss.item()\n            recons_loss += sparsity_loss\n        return recons_loss\n\n    def train_epoch(self, epoch, train_loader):\n        \"\"\"\n            Does mostly accounting. The actual trianing is done by the train_batch method.\n        \"\"\"\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        r_crop = transforms.RandomCrop(32, padding=4)\n        tensor_to_image = transforms.ToPILImage()\n        totensor = transforms.ToTensor()\n\n        # switch to train mode\n        self.model.train()\n\n        end = time.time()\n\n        for i, (inputs, targets) in enumerate(train_loader, 0):\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            # targets = []\n            # for index in indexes:\n            #     targets.append(classes[index])\n\n            # print (targets)\n            # print (indexes.shape)\n            # # indexes = indexes.view(-1,1)\n            # print (indexes.shape)\n            # print (inputs.shape)\n            # targets_onehot = torch.LongTensor(inputs.shape[0], 50)\n            # targets_onehot.zero_()\n            # targets = targets_onehot.scatter_(1,indexes,1)\n            targets = Variable(targets.long())\n            # print (targets)\n            # targets = Variable(torch.stack(targets,0))\n\n            # inputs_ = []\n            # targets_ = []\n\n            # for j in range(inputs.shape[0]):\n            #     inputs_crop = totensor(r_crop(tensor_to_image(inputs[j])))\n            #     # inputs90 = inputs[j].transpose(2,1).flip(1)\n            #     # inputs180 = inputs90.transpose(2,1).flip(1)\n            #     # inputs270 =  inputs180.transpose(2,1).flip(1)\n            #     inputs_ += [inputs[j], inputs_crop]#, inputs90, inputs180, inputs270]\n            #     targets_ += [targets[j] for _ in range(2)]\n\n            # inputs = Variable(torch.stack(inputs_,0))\n            # targets = Variable(torch.stack(targets_,0))\n\n            # get the inputs\n            if self.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            inputs, targets = Variable(inputs), Variable(targets)\n\n            # if self.reset_lstm:\n            #     self.model.zero_grad()\n            #     self.model.parametrizer.hidden = self.model.parametrizer.init_hidden()# detaching it from its history on the last instance.\n\n            outputs, loss, loss_dict = self.train_batch(inputs, targets)\n            loss_dict['iter'] = i + (len(train_loader)*epoch)\n            \n            # the dict here\n            self.loss_history.append(loss_dict)\n\n            # measure accuracy and record loss\n            if self.nclasses > 4:\n                prec1, prec5 = self.accuracy(outputs.data, targets.data, topk=(1, 5))\n            elif self.nclasses in [3,4]:\n                prec1, prec5 = self.accuracy(outputs.data, targets.data, topk=(1,self.nclasses))\n            else:\n                prec1, prec5 = self.binary_accuracy(outputs.data, targets.data), [100]\n\n            #\n            # if self.nclasses <= 2:\n            #     prec1 = self.binary_accuracy(outputs.data, targets.data)\n            #     prec5 = [100]\n            # else:\n            #     prec1, prec5 = self.accuracy(outputs.data, targets.data, topk=(1,5))\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec1[0], inputs.size(0))\n            top5.update(prec5[0], inputs.size(0))\n\n             # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % self.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]  '\n                      'Time {batch_time.val:.2f} ({batch_time.avg:.2f})  '\n                      #'Data {data_time.val:.2f} ({data_time.avg:.2f})  '\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})  '\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                       epoch, i, len(train_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n\n    def validate(self, val_loader, fold = None):\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to evaluate mode\n        self.model.eval()\n\n        end = time.time()\n        for i, (inputs, targets) in enumerate(val_loader, 0):\n            # get the inputs\n            # normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n            #                                   std=[0.5, 0.5, 0.5])\n            # for i in range(inputs.shape[0]):\n            #     temp = TF.to_pil_image(inputs[i,:,:,:])\n            #     temp = TF.to_tensor(temp)\n            #     inputs[i,:,:,:] = normalize(temp)\n\n            # inputs = gaussian_noise(inputs)\n            # inputs = speckle_noise(inputs)\n            targets = Variable(targets.long())\n\n\n            if self.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            input_var = torch.autograd.Variable(inputs, volatile=True)\n            target_var = torch.autograd.Variable(targets, volatile=True)\n\n            # compute output\n            output, _, _, _ = self.model(input_var)\n            loss   = self.prediction_criterion(output, target_var)\n\n            # measure accuracy and record loss\n            if self.nclasses > 4:\n                prec1, prec5 = self.accuracy(output.data, targets, topk=(1, 5))\n            elif self.nclasses == 3:\n                prec1, prec5 = self.accuracy(output.data, targets, topk=(1,3))\n            else:\n                prec1, prec5 = self.binary_accuracy(output.data, targets), [100]\n\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec1[0], inputs.size(0))\n            top5.update(prec5[0], inputs.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % self.print_freq == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses,\n                       top1=top1, top5=top5))\n\n        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n              .format(top1=top1, top5=top5))\n\n        return top1.avg\n\n    def evaluate(self, test_loader, fold = None):\n        self.model.eval()\n        test_loss = 0\n        correct = 0\n        correct_aux=0\n        y = np.zeros(10) \n        # y=[]\n        for i, (data, targets) in enumerate(test_loader, 0):\n          if i==0:\n            # data = gaussian_noise(data)\n            # data = speckle_noise(data)\n            # targets = []\n            # for index in indexes:\n            #     targets.append(classes[index])\n            count=0\n            #for i, (x,y) in enumerate(data,targets):\n             # if count==0:\n            targets = Variable(targets.long())\n            if self.cuda:\n              data, targets = data.cuda(), targets.cuda()\n\n            data, targets = Variable(data, volatile=True), Variable(targets)\n\n            # if self.reset_lstm:\n            #     self.model.zero_grad()\n            #     self.model.parametrizer.hidden = self.model.parametrizer.init_hidden()# detaching it from its history on the last instance.\n            \n            output, output_aux, concepts, _ = self.model(data)\n                #test_loss += self.prediction_criterion(output, targets.view(targets.size(0))).data[0]\n            test_loss += self.prediction_criterion(output, targets).item()\n\n                # if targets == 7:\n                #     concepts_test = concepts[0].cpu().detach().numpy()\n                #     # concepts_norm = scale(concepts_test, 0, 1)\n                #     # print (concepts_test.shape)\n                #     print (i)\n                #     # y.append(concepts_test)\n                #     y = y + concepts_test\n                #     save_image(data[0], os.path.join('/home/cs16resch11006/CIFAR10/scripts/test3/', str(i)+'.png'))\n\n            print (concepts)\n            # concepts_test = concepts[0].cpu().detach().numpy()\n            # concepts_norm = scale(concepts_test, 0, 1)\n            # # concepts_topp = np.percentile(concepts_test, 20, axis=0)\n            # # # print (concepts_topp)\n            # # print (concepts_norm<=0.99)\n            # concepts_least = concepts_test*(concepts_norm<=0.99)\n            # # # concepts_most = concepts_test*(concepts_norm>0.8)\n            # # concepts_least = concepts_test*(concepts_test>concepts_topp)\n            # # print (concepts_least) \n            # # print (i)\n            # output_interv = self.model.aggregator(torch.from_numpy(concepts_least).cuda())\n            # # # print (output_interv)\n\n            # save_image(data[0], os.path.join('/home/cs16resch11006/CIFAR10/scripts/test/', str(i)+'.png'))\n            # # sys.exit()\n\n            #print(output)\n            if self.nclasses == 2:\n                pred = output.data.round()\n            else:\n                pred = output.data.max(1)[1] # get the index of the max log-probability\n                print(\"pred\",pred)\n                pred_aux = output_aux.data.max(1)[1]\n                print(\"pred_aux\",pred_aux)\n            # if pred!=targets and pred_aux==targets:\n                # save_image(data[0], os.path.join('/home/cs16resch11006/CIFAR10/scripts/test/', str(i)+'.png'))\n                # print (concepts)\n                # print (concepts_norm<=0.99)\n                # print (concepts_least)\n                # print(pred, pred_aux, targets)\n                # print (i)\n            correct += pred.eq(targets.data).cpu().sum()\n            correct_aux += pred_aux.eq(targets.data).cpu().sum()\n\n        # y = np.asarray(y)\n        print (y)\n        # print (np.argmax(y1,0))\n        # print (np.argsort(-y1,axis=0)[:4])\n        # print (np.argpartition(y1, -5, axis=0)[:, -5:])\n        test_loss = test_loss\n        test_loss /= len(test_loader) # loss function already averages over batch size\n        fold = '' if (fold is None) else ' (' + fold + ')'\n        acc = 100. * correct / len(test_loader.dataset)\n        acc_aux = 100. * correct_aux / len(test_loader.dataset)\n        print('\\nEvaluation{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            fold, test_loss, correct, len(test_loader.dataset),acc))\n        print('\\nEvaluation_auxiliary{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            fold, test_loss, correct_aux, len(test_loader.dataset),acc_aux))\n        return acc\n\n    def binary_accuracy(self, output, target):\n        \"\"\"Computes the accuracy\"\"\"\n        return torch.Tensor(1).fill_((output.round().eq(target)).float().mean()*100)\n\n    def accuracy(self, output, target, topk=(1,)):\n        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n        maxk = max(topk)\n        batch_size = target.size(0)\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n        res = []\n        for k in topk:\n            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n    def accuracy_per_class(self, model, test_loader, classes):\n        \"\"\" TODO: Homogenize with accuracy style and synbtax\"\"\"\n        n = len(classes)\n        class_correct = list(0. for i in range(n))\n        class_total = list(0. for i in range(n))\n        confusion_matrix = ConfusionMeter(n) #I have 2 classes here\n        for data in test_loader:\n            inputs, labels = data\n            if self.cuda:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(Variable(inputs))\n            _, predicted = torch.max(outputs.data, 1)\n            c = (predicted == labels).squeeze()\n            confusion_matrix.add(predicted, labels)\n            for i in range(labels.size()[0]):\n                label = labels[i]\n                class_correct[label] += c[i]\n                class_total[label] += 1\n\n    def plot_losses(self, save_path = None):\n        loss_types = [k for k in self.loss_history[0].keys() if k != 'iter']\n        losses = {k: [] for k in loss_types}\n        iters  = []\n        for e in self.loss_history:\n            iters.append(e['iter'])\n            for k in loss_types:\n                losses[k].append(e[k])\n        fig, ax = plt.subplots(1,len(loss_types), figsize = (4*len(loss_types), 5))\n        if len(loss_types) == 1:\n            ax = [ax] # Hacky, fix\n        for i, k in enumerate(loss_types):\n            ax[i].plot(iters, losses[k])\n            ax[i].set_title('Loss: {}'.format(k))\n            ax[i].set_xlabel('Iters')\n            ax[i].set_ylabel('Loss')\n        if save_path is not None:\n            plt.savefig(save_path + '/training_losses.pdf', bbox_inches = 'tight', format='pdf', dpi=300)\n        #plt.show(block=False)\n\n\n\"\"\"\n\n    Since the train_batch method abstracts away most of the details of training,\n    just need to specifiy that for every training scheme. Everything else is\n    shared.\n\n\"\"\"\n\n\n\nclass VanillaClassTrainer(ClassificationTrainer):\n    \"\"\"\n        The simplest classification trainer. No regularization, just normal\n        prediction loss.\n    \"\"\"\n    def __init__(self, model, args):\n        super().__init__(model, args)\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        self.optimizer.zero_grad()\n        pred = self.model(inputs)\n\n        # Loss\n        try:\n            pred_loss       = self.prediction_criterion(pred, targets)\n        except:\n            pdb.set_trace()\n        all_losses = {'prediction': pred_loss.data[0]}\n        if self.learning_h:\n            h_loss = self.concept_learning_loss(inputs, all_losses)\n            loss = pred_loss + h_loss\n        else:\n            loss = pred_loss\n\n        # Keep track of losses\n        #self.loss_history.append(all_losses)\n\n        # Backpropagation\n        loss.backward()\n        self.optimizer.step()\n\n        return pred, loss, all_losses\n\n\nclass CLPenaltyTrainer(ClassificationTrainer):\n    \"\"\"\n        Uses the penalty:\n\n            ( || dy/dx || - || theta ||)^2\n\n    \"\"\"\n    def __init__(self, model, args):\n        super().__init__(model, args)\n        self.lambd =  args.theta_reg_lambda #regularization strenght\n        self.R     = 1     # radius for lipschitz locality\n        self.reconst_criterion = nn.MSELoss()\n\n        if self.cuda: self.reconst_criterion = self.reconst_criterion.cuda()\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        # Init\n        self.optimizer.zero_grad()\n\n        inputs.requires_grad = True\n\n        # Predict\n        pred = self.model(inputs)\n\n        # Calculate loss\n        #loss = F.cross_entropy(y_pred, target)\n        pred_loss       = self.prediction_criterion(pred, targets)\n        torch.autograd.backward(pred_loss, create_graph=True)\n        #update1 = model.weight.grad.data.clone()\n\n        grad_penalty = self.calc_crosslip_penalty(self.model.parametrizer, inputs)#, pred)\n        grad_penalty.backward() # this will be added to the grads w.r.t. the loss\n\n        #print(pred_loss.data[0], grad_penalty.data[0])\n\n        loss = pred_loss + self.lambd*grad_penalty\n\n        self.loss_history.append([pred_loss.data[0], grad_penalty.data[0]])\n        self.optimizer.step()\n\n        return pred, loss\n\n    def calc_crosslip_penalty(self, net, x):\n        thetas = net(x)\n        nclass = thetas.size(1)\n        i = 0\n        grad_outputs =  + 1\n        #print(grad_outputs)\n\n        # gradients =torch.autograd.grad(outputs=thetas,inputs=x,\n        #                                   grad_outputs = torch.ones(thetas.size())  if self.cuda else torch.ones(\n        #                                   thetas.size()),\n        #                                   create_graph=True)[0].squeeze()\n\n\n        #print(gradients.size())\n        d_thetas = [] #torch.zeros(nclass)\n        for i in range(nclass):\n            grad_class_i = torch.autograd.grad(outputs=thetas[:,i],inputs=x,\n                                              grad_outputs = torch.ones(thetas[:,i].size())  if self.cuda else torch.ones(\n                                              thetas[:,i].size()),\n                                              create_graph=True, retain_graph = True, only_inputs=True)[0].squeeze()\n            d_thetas.append(grad_class_i.view(x.size(0), -1)) # B x all input dim\n\n        total = Variable(torch.zeros(x.size(0)))\n        for i in range(nclass):\n            for j in range(nclass):\n                total += (d_thetas[i] - d_thetas[j]).norm(dim=1).squeeze()**2 # B x 1\n\n        #penalty = total/(nclass**2 * x.size(0))\n        penalty = total.mean()/(nclass**2) # Mean over examples in batch.\n        return penalty\n\nclass GradPenaltyTrainer(ClassificationTrainer):\n    \"\"\" Gradient Penalty Trainer. Depending on the type, uses different penalty:\n             Mode 1. || df/dx - theta ||^2\n             Mode 2. || dtheta/dx  || / || dh / dx ||\n             Mode 3. || df/dx - dh/dx*theta  || (=  || dth/dx*h  || )\n    \"\"\"\n    def __init__(self, model, typ):\n        super().__init__(model)\n\n        self.lambd = 1e-6 #regularization strenght\n        self.reconst_criterion = nn.MSELoss()\n        self.penalty_type = typ\n        self.norm = 2\n\n        if self.cuda: self.reconst_criterion = self.reconst_criterion.cuda()\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        # Init\n        self.optimizer.zero_grad()\n        #self.model.zero_grad()\n\n        inputs.requires_grad = True\n\n        # Predict\n        pred1, pred2, _, _ = self.model(inputs)\n\n        # Calculate loss\n        pred_loss       = self.prediction_criterion(pred1, targets)\n        all_losses = {'prediction': pred_loss.item()}\n        # aux_loss = self.prediction_criterion(pred2, targets)\n        aux_loss = ((pred1.data.clone() - pred2)**2).mean()\n        # aux_loss = self.criterion_kl(F.log_softmax(pred2, dim=1),\n                                       # F.softmax(pred1, dim=1))\n        all_losses['auxiliary_prediction'] = aux_loss.item()\n        if self.learning_h:\n            h_loss = self.concept_learning_loss(inputs, all_losses)\n            loss = pred_loss + 0.0001 * aux_loss # + 0.0001 * h_loss\n            # loss = pred_loss\n        else:\n            loss = pred_loss\n\n        #torch.autograd.backward(pred_loss, create_graph=True)\n        #print(pred.grad.size())\n        #update1 = model.weight.grad.data.clone()\n\n        # if self.penalty_type == 1:\n            \n        #     #raise NotImplementedError('Fix this')\n        #     #  || df/dx - theta ||)^2\n        #     #dTh = self.compute_parametrizer_jacobian(inputs)\n        #     dF = torch.autograd.grad(outputs=pred.mean(),inputs=inputs, create_graph=True)[0]\n        #     pdb.set_trace()\n        #     grad_penalty =  (dTh - self.model.thetas).norm(self.norm) #.pow(2)\n        # elif self.penalty_type == 2:\n        #     #     || dtheta/dx  || / || dh / dx ||\n        #     dTh = self.compute_parametrizer_jacobian(inputs)\n        #     if self.learning_h:\n        #         dH  = self.compute_conceptizer_jacobian(inputs)\n        #         grad_penalty = dTh.norm(self.norm)/dH.norm(self.norm)\n        #     else:\n        #         # We're working with inputs, dH is identity\n        #         grad_penalty = dTh.norm(self.norm)/inputs.size(0)**(0.5)\n        # else:\n        #     # (V1)  || dh/dx*theta - df/dx  || =  || dth/dx*h  ||  (V2)\n        #     # For V1:\n        #     dF = compute_jacobian(inputs, pred2)#  pred.squeeze())  # Squeeze wwas braeking binary case\n        #     dF_dH = compute_jacobian(self.model.concepts, pred2)\n        #     # print (dF.shape)\n        #     # print (dF_dH.shape)\n        #     if self.learning_h:\n        #         dH  = self.compute_conceptizer_jacobian(inputs)\n        #         # print (dH.shape)\n        #         # prod = torch.bmm(dH, self.model.thetas)\n        #         prod = torch.bmm(dH, dF_dH)\n        #     else:\n        #         # We're working with inputs, dH is identity\n        #         prod = self.model.thetas\n        #         if self.model.conceptizer.add_bias:\n        #             # Need to take pad with zero derivatives for constant bias term\n        #             pad = (0,0,0,1) # Means pad to next to last dim, 0 at beginning, 1 at end\n        #             dF = F.pad(dF, pad, mode = 'constant', value = 0)\n        #     ## For V2:\n        #     #dTh = self.compute_parametrizer_jacobian(inputs).squeeze()\n        #     # Then?? Need to do for, bmm does not do 4D. Maybe trhough sum approach?\n        #     grad_penalty = (prod - dF).norm(self.norm) #.pow(2)\n\n        # all_losses['grad_penalty'] = grad_penalty.data[0]\n\n        #grad_penalty.backward() # this will be added to the grads w.r.t. the loss\n\n        # print(grad_penalty.data[0])\n        # loss = pred_loss\n        # loss = pred_loss + self.lambd*grad_penalty\n        # loss = loss + self.lambd*grad_penalty\n        loss.backward()\n\n        #self.loss_history.append([pred_loss.data[0], grad_penalty.data[0]])\n        self.optimizer.step()\n\n        return pred1, loss, all_losses\n\n    def compute_parametrizer_jacobian(self, x):\n        thetas  = self.model.thetas\n        nclass  = self.nclasses\n        if self.norm == 1:\n            JTh = compute_jacobian_sum(x,thetas.squeeze()).unsqueeze(-1)\n        elif nclass == 1:\n            JTh = compute_jacobian(x, thetas[:,:,0])\n        else:\n            JTh = []\n            for i in range(nclass):\n                JTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n            JTh = torch.cat(JTh, 3)\n            assert list(JTh.size()) == [x.size(0), x.view(x.size(0),-1).size(1), thetas.size(-2)]\n        return JTh\n\n    def compute_conceptizer_jacobian(self, x):\n        h = self.model.concepts\n        Jh = compute_jacobian(x, h.squeeze())\n        assert list(Jh.size()) == [x.size(0), x.view(x.size(0),-1).size(1), h.size(1)]\n        return Jh\n\n    def compute_fullmodel_gradient(self, x, ypred):\n        grad = torch.autograd.grad(ypred, x,\n                           grad_outputs=ypred.data.new(ypred.shape).fill_(1),\n                           create_graph=True)[0]\n        return grad\n\n\n    #\n    #\n    # def calc_gradient_penalty_1(self, net,x,y):\n    #     \"\"\"\n    #         ( || df/dx - theta ||)^2\n    #\n    #     \"\"\"\n    #     \n    #     g = torch.autograd.grad(outputs=y.mean(),inputs=x, create_graph=True)[0]\n    #     print(g.size())\n    #     print(net.thetas.size())\n    #\n    #     thetas = net.parametrizer(x)\n    #     nclass = thetas.size(-1)\n    #     DTh = []\n    #     for i in range(nclass):\n    #         DTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n    #\n    #     DTh = torch.cat(DTh, 3)\n    #\n    #\n    #     #J = compute_jacobian(inputs, self.model.thetas)\n    #     print(DTh.size())\n    #\n    #     diff = (g - net.thetas).norm().pow(2)\n    #     return diff\n    #\n    # def calc_gradient_penalty_2(self,model,x,y, norm = 1):\n    #     \"\"\"\n    #         Uses the penalty.\n    #\n    #              || dtheta/dx  || / || dh / dx ||\n    #     \"\"\"\n    #     \n    #     # the variables not the data\n    #     thetas = model.thetas #model.parametrizer(x)\n    #     # if True:\n    #     #     return y.norm()\n    #     nclass = thetas.size(-1)\n    #     if norm == 1:\n    #         DTh = compute_jacobian_sum(x,thetas.squeeze()).unsqueeze(-1)\n    #     else:\n    #         DTh = []\n    #         for i in range(nclass):\n    #             DTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n    #         DTh = torch.cat(DTh, 3)\n    #\n    #     #h,_ = model.conceptizer(x)\n    #     h = model.concepts\n    #     Jh = compute_jacobian(x, h.squeeze())\n    #     assert list(Jh.size()) == [x.size(0), x.view(x.size(0),-1).size(1), h.size(1)]\n    #     ratio = DTh.norm(norm)/Jh.norm(norm)\n    #     #DTh.data.zero_()\n    #     #Jh.data.zero_()\n    #     return ratio#**2\n    #\n    # def calc_gradient_penalty_3(self, net,x,y, norm =2):\n    #     \"\"\"\n    #         Uses the penalty.\n    #\n    #              || dh/dx*theta - df/dx  || =  || dth/dx*h  ||\n    #\n    #     \"\"\"\n    #     \n    #     # the variables not the data\n    #     thetas = model.thetas #model.parametrizer(x)\n    #     # if True:\n    #     #     return y.norm()\n    #     nclass = thetas.size(-1)\n    #     if norm == 1:\n    #         DTh = compute_jacobian_sum(x,thetas.squeeze()).unsqueeze(-1)\n    #     else:\n    #         DTh = []\n    #         for i in range(nclass):\n    #             DTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n    #         DTh = torch.cat(DTh, 3)\n    #\n    #     print(DTh.size())\n    #\n    #\n    #     g = torch.autograd.grad(outputs=y.mean(),inputs=x, create_graph=True)[0]\n    #     print(g.size())\n    #     print(net.thetas.size())\n    #\n    #     #h,_ = model.conceptizer(x)\n    #     h = model.concepts\n    #     Jh = compute_jacobian(x, h.squeeze())\n    #     assert list(Jh.size()) == [x.size(0), x.view(x.size(0),-1).size(1), h.size(1)]\n    #     ratio = DTh.norm(norm)/Jh.norm(norm)\n    #     #DTh.data.zero_()\n    #     #Jh.data.zero_()\n    #     return ratio#**2\n\n### DEPRECATED\n\n\n\nclass HLearningClassTrainer(ClassificationTrainer):\n    \"\"\"\n        Trainer for end-to-end training of conceptizer H\n    \"\"\"\n    def __init__(self, model, args):\n        super().__init__(model, args)\n        self.sparsity = args.h_sparsity\n        self.reconst_criterion = nn.MSELoss() \n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        pred = self.model(inputs)\n\n        # Loss\n        pred_loss       = self.prediction_criterion(pred, targets)\n        reconst_loss    = self.reconst_criterion(self.model.recons,\n                            Variable(inputs.data, requires_grad = False))\n\n        loss = pred_loss + reconst_loss\n\n        # Sparsity penalty on encoded H\n        if self.sparsity is not None:\n            sparsity_loss   = self.model.h_norm_l1.mul(self.sparsity)\n            loss += sparsity_loss\n\n        # Keep track of losses\n        self.loss_history.append([pred_loss.data[0], reconst_loss.data[0]])\n\n        # Backpropagation\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return pred, loss\n\n\nclass GradPenaltyTrainer_old(ClassificationTrainer):\n    \"\"\"\n        Uses the penalty.\n\n            ( || df/dx - theta ||)^2\n\n    \"\"\"\n    def __init__(self, model, args):\n        super().__init__(model, args)\n\n        self.lambd = args.lambd if ('lambd' in args) else 1e-6 #regularization strenght\n        self.reconst_criterion = nn.MSELoss()\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        # Init\n        self.optimizer.zero_grad()\n\n        inputs.requires_grad = True\n\n        # Predict\n        pred = self.model(inputs)\n\n        # Calculate loss\n        #loss = F.cross_entropy(y_pred, target)\n        pred_loss       = self.prediction_criterion(pred, targets)\n        torch.autograd.backward(pred_loss, create_graph=True)\n        #update1 = model.weight.grad.data.clone()\n\n        grad_penalty = self.calc_gradient_penalty(self.model, inputs, pred)\n        grad_penalty.backward() # this will be added to the grads w.r.t. the loss\n\n        loss = pred_loss + self.lambd*grad_penalty\n\n        self.loss_history.append([pred_loss.data[0], grad_penalty.data[0]])\n        self.optimizer.step()\n\n        return pred, loss\n\n    def calc_gradient_penalty(self, net,x,y):\n        \n        g = torch.autograd.grad(outputs=y.mean(),inputs=x, create_graph=True)[0]\n        print(g.size())\n        print(net.thetas.size())\n\n        thetas = net.parametrizer(x)\n        nclass = thetas.size(-1)\n        DTh = []\n        for i in range(nclass):\n            DTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n\n        DTh = torch.cat(DTh, 3)\n\n\n        #J = compute_jacobian(inputs, self.model.thetas)\n        print(DTh.size())\n\n        diff = (g - net.thetas).norm().pow(2)\n        return diff\n\n\n\nclass GradPenaltyTrainer3(ClassificationTrainer):\n    \"\"\"\n        Uses the penalty.\n\n             || dh/dx*theta - df/dx  || =  || dth/dx*h  ||\n\n    \"\"\"\n    def __init__(self, model, args):\n        super().__init__(model, args)\n\n        self.lambd = args.lambd if ('lambd' in args) else 1e-6 #regularization strenght\n        self.reconst_criterion = nn.MSELoss()\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        # Init\n        self.optimizer.zero_grad()\n        #self.model.zero_grad()\n\n        inputs.requires_grad = True\n\n        # Predict\n        pred = self.model(inputs)\n\n        # Calculate loss\n        #loss = F.cross_entropy(y_pred, target)\n        pred_loss       = self.prediction_criterion(pred, targets)\n        #torch.autograd.backward(pred_loss, create_graph=True)\n        #update1 = model.weight.grad.data.clone()\n\n        grad_penalty = self.calc_gradient_penalty(self.model, inputs, pred, norm = 1)\n        #grad_penalty.backward() # this will be added to the grads w.r.t. the loss\n\n        print(grad_penalty.data[0])\n        loss = pred_loss + self.lambd*grad_penalty\n        loss.backward()\n\n        self.loss_history.append([pred_loss.data[0], grad_penalty.data[0]])\n        self.optimizer.step()\n\n        return pred, loss\n\n    def calc_gradient_penalty(self,model,x,y, norm = 1):\n        \n        # the variables not the data\n        thetas = model.thetas #model.parametrizer(x)\n        # if True:\n        #     return y.norm()\n        nclass = thetas.size(-1)\n        if norm == 1:\n            DTh = compute_jacobian_sum(x,thetas.squeeze()).unsqueeze(-1)\n        else:\n            DTh = []\n            for i in range(nclass):\n                DTh.append(compute_jacobian(x, thetas[:,:,i]).unsqueeze(-1))\n            DTh = torch.cat(DTh, 3)\n\n        #h,_ = model.conceptizer(x)\n        h = model.concepts\n        Jh = compute_jacobian(x, h.squeeze())\n        assert list(Jh.size()) == [x.size(0), x.view(x.size(0),-1).size(1), h.size(1)]\n        ratio = DTh.norm(norm)/Jh.norm(norm)\n        #DTh.data.zero_()\n        #Jh.data.zero_()\n        return ratio#**2\n\n\n\n\n### =========================   DEPRECATED   ================================ ###\n\n\n#### Legacy\nclass NormalTrainer_old():\n    \"\"\" Trainer for supervised digit classification in a framework consisting\n        of two parts:\n            M - model\n    \"\"\"\n    def __init__(self, M, loss_type = 'ce', opt = 'adam', lr = 0.0002, cuda = False, log_interval = 100):\n        super(NormalTrainer, self).__init__()\n        self.M = M\n        self.prediction_criterion = F.nll_loss\n        optim_betas = (0.9, 0.999)\n        if opt == 'adam':\n            self.optimizer = optim.Adam(self.M.parameters(), lr=lr, betas=optim_betas)\n        elif opt == 'rmsprop':\n            #lrD, lrG = 5e-5, 5e-5 # Original WGAN code has 5e-5\n            self.optimizer = optim.RMSprop(self.M.parameters(), lr = lr)\n        elif opt == 'sgd':\n            self.optimizer = optim.SGD(self.M.parameters(), lr = lr, momentum=0.9)\n\n\n        self.cuda = cuda\n        self.log_interval = log_interval\n\n    def train(self, train_loader, test_loader = None, epochs = 10):\n        self.M.train()\n        losses = []\n        for epoch in range(epochs):\n            for batch_idx, (inputs, targets) in enumerate(train_loader):\n                if self.cuda:\n                    inputs, targets = inputs.cuda(), targets.cuda()\n\n                inputs, targets = Variable(inputs), Variable(targets)\n\n                # Init\n                self.optimizer.zero_grad()\n\n                # Predict\n                pred = self.M(inputs)\n\n                # Loss\n                loss = self.prediction_criterion(pred, targets)\n                losses.append(loss.data[0])\n\n                # Backpropagation\n                loss.backward()\n                self.optimizer.step()\n\n                #loss = self.train_batch(inputs, targets)  ### Maybe abstract away differences in refularized traininf in different train_batch methods??\n\n                if batch_idx % self.log_interval == 1:\n                    print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        epoch,\n                        batch_idx * len(inputs),\n                        len(train_loader.dataset),\n                        100. * batch_idx / len(train_loader),\n                        loss.data[0]),\n                        end='')\n            print()\n\n\n            if test_loader:\n                self.evaluate(test_loader)\n    #\n    # def train_batch(self, batch, targets):\n    #     self.M_optimizer.zero_grad()\n    #     self.C_optimizer.zero_grad()\n    #     pred_targets = self.C(self.M(batch))\n    #     loss        = self.criterion(pred_targets, targets.view(targets.size(0)))\n    #     loss.backward()\n    #     self.M_optimizer.step()\n    #     self.C_optimizer.step()\n    #     return loss\n\n    def evaluate(self, test_loader):\n        self.M.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            if self.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data, volatile=True), Variable(target)\n            output = self.M(data)\n            test_loss += self.prediction_criterion(output, target.view(target.size(0))).data[0]\n            #print(output)\n            pred = output.data.max(1)[1] # get the index of the max log-probability\n            #print(pred, target)\n            correct += pred.eq(target.data).cpu().sum()\n\n        test_loss = test_loss\n        test_loss /= len(test_loader) # loss function already averages over batch size\n        print('\\nEvaluation: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            test_loss, correct, len(test_loader.dataset),\n            100. * correct / len(test_loader.dataset)))\n\n\n\nclass HLearningTrainer():\n    \"\"\" Trainer for\n    \"\"\"\n    def __init__(self, M, loss_type = 'ce', opt = 'adam', lr = 0.0002, cuda = False, log_interval = 100):\n        super(HLearningTrainer, self).__init__()\n        self.M = M\n        #self.prediction_criterion = F.nll_loss\n        self.prediction_criterion    = nn.NLLLoss()\n        self.reconst_criterion = nn.MSELoss()\n\n        optim_betas = (0.9, 0.999)\n        if opt == 'adam':\n            self.optimizer = optim.Adam(self.M.parameters(), lr=lr, betas=optim_betas)\n        elif opt == 'rmsprop':\n            #lrD, lrG = 5e-5, 5e-5 # Original WGAN code has 5e-5\n            self.optimizer = optim.RMSprop(self.M.parameters(), lr = lr)\n        elif opt == 'sgd':\n            self.optimizer = optim.SGD(self.M.parameters(), lr = lr, momentum=0.9)\n\n        self.cuda = cuda\n        self.log_interval = log_interval\n\n\n    def train_batch(self, inputs, targets):\n        \"\"\" inputs, targets already variables \"\"\"\n        # Loss\n        pred_loss       = self.prediction_criterion(pred, targets)\n        reconst_loss    = self.reconst_criterion(self.M.recons,\n                            Variable(inputs.data, requires_grad = False))\n        loss = pred_loss + reconst_loss\n\n        # Keep track of losses\n        pred_losses.append(pred_loss.data[0])\n        reconst_losses.append(reconst_loss.data[0])\n\n        # Backpropagation\n        loss.backward()\n\n\n    def train(self, train_loader, test_loader = None, epochs = 10):\n        pred_losses   = []\n        reconst_losses = []\n        for epoch in range(epochs):\n            self.M.train()\n            for batch_idx, (inputs, targets) in enumerate(train_loader):\n                if self.cuda:\n                    inputs, targets = inputs.cuda(), targets.cuda()\n\n                inputs, targets = Variable(inputs), Variable(targets)\n\n                # Init\n                self.optimizer.zero_grad()\n\n                # Predict\n                pred = self.M(inputs)\n\n                self.train_batch(inputs, targets)\n\n                self.optimizer.step()\n\n                #loss = self.train_batch(inputs, targets)  ### Maybe abstract away differences in refularized traininf in different train_batch methods??\n\n                if batch_idx % self.log_interval == 1:\n                    print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tPred Loss: {:.6f}\\tRecons Loss: {:.6f}'.format(\n                        epoch,\n                        batch_idx * len(inputs),\n                        len(train_loader.dataset),\n                        100. * batch_idx / len(train_loader),\n                        pred_loss.data[0],\n                        reconst_loss.data[0]),\n                        end='')\n            print()\n\n            if test_loader:\n                self.evaluate(test_loader)\n    #\n    # def train_batch(self, batch, targets):\n    #     self.M_optimizer.zero_grad()\n    #     self.C_optimizer.zero_grad()\n    #     pred_targets = self.C(self.M(batch))\n    #     loss        = self.criterion(pred_targets, targets.view(targets.size(0)))\n    #     loss.backward()\n    #     self.M_optimizer.step()\n    #     self.C_optimizer.step()\n    #     return loss\n\n    def evaluate(self, test_loader):\n        self.M.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            if self.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data, volatile=True), Variable(target)\n            output = self.M(data)\n            test_loss += self.prediction_criterion(output, target.view(target.size(0))).data[0]\n            #print(output)\n            pred = output.data.max(1)[1] # get the index of the max log-probability\n            #print(pred, target)\n            correct += pred.eq(target.data).cpu().sum()\n\n        test_loss = test_loss\n        test_loss /= len(test_loader) # loss function already averages over batch size\n        print('\\nEvaluation: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            test_loss, correct, len(test_loader.dataset),\n            100. * correct / len(test_loader.dataset)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}